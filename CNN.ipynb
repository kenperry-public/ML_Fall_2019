{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{\\tp}{\\mathbf{{(t)}}}\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\kernel}{\\mathbf{k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import cnn_helper\n",
    "%aimport cnn_helper\n",
    "cnnh = cnn_helper.CNN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks (CNN): HIgh Level\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li> A single unit in Fully Connected (FC) Layer identifies the presence/absence of a single feature spanning the <b>entire</b> input</li>\n",
    "        <li>A single \"kernel\" in a Convolutional Layer identifies the presence/absence of a single feature</li>\n",
    "        <ul>\n",
    "            <li>Whose size is a fraction of the entire input</li>\n",
    "            <li>At <b>each</b> sub-span of the input</li>\n",
    "        </ul>\n",
    "        \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**\n",
    "- FC: is the input image the digit \"8\"\n",
    "- CNN: are there one or more small \"8\"'s in the input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have seen how the Fully Connected (FC) layer performs a template-matching on the layer's inputs.\n",
    "$$\n",
    "\\y_\\llp = a_\\llp ( \\W_\\llp \\y_{(\\ll-1)} + b )\n",
    "$$\n",
    "\n",
    "- Each element of $\\y_{(\\ll-1)}$ is independent\n",
    "    - there is no relationship between $\\y_{(\\ll-1), j}$ and $\\y_{(\\ll-1), j+1}$\n",
    "    - even though they are adjacent in the vector ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To see the lack of relationship:\n",
    "\n",
    "Let $\\text{perm}$ be a random ordering of the integers in the range $[1 \\ldots n]$.\n",
    "\n",
    "Then\n",
    "- $\\x[ \\text{perm} ]$ is a permutation of input $\\x$\n",
    "- $\\Theta[ \\text{perm} ]$ is the corresponding permutation of parametrs $\\Theta$.\n",
    "\n",
    "$$\n",
    "\\Theta^T \\cdot \\x = \\x[ \\text{perm} ] \\cdot \\Theta[ \\text{perm} ]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So a FC layer cannot take advantage of any explicit ordering among the input elements\n",
    "- timeseries of prices\n",
    "- adjacent pixels in an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another issue with an FC layer:\n",
    "- The \"template\" $\\W_\\llp$ matches the full length of the input $\\y_{(\\ll-1)}$ \n",
    "\n",
    "There are cases where we might want to discover a feature\n",
    "- whose length is less than $n$\n",
    "- that occurs *anywhere* in the input, rather than at a fixed location\n",
    "\n",
    "For example\n",
    "- a spike in a timeseries\n",
    "- the presence of an \"eye\" in an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Both these questions motivate the notion of convolutional matching\n",
    "- small templates\n",
    "- that are slid over the entire input\n",
    "\n",
    "By sliding the pattern over the entire input\n",
    "- we can detect the existence of the feature somewhere in the input\n",
    "- we can localize its location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>CNN convolution</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/CNN_convolution_1.jpg\" width=900></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We place $(3 \\times 3)$ Kernel (weight matrix) on the inputs ($\\y_{(0)}$, output of layer 0)\n",
    "- Performs dot product\n",
    "- Produces Layer 1 output ($\\y_{(1)}$) feature labelled $1$\n",
    "\n",
    "Slide the Kernel up and repeat:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>CNN convolution</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/CNN_convolution_2.jpg\" width=900></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The dot product *using the identical kernel weights* produces output ($\\y_{(1)}$) feature labelled $2$\n",
    "\n",
    "Repeat, centering the Kernel over each feature in $\\y_{(0)}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>CNN convolution</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/CNN_convolution_3.jpg\" width=900></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The output of a convolution is of similar size as the input\n",
    "- detects a specific feature *at each input location*\n",
    "\n",
    "So, for example, if there are\n",
    "- three spikes: will detect the \"is a spike\" feature in 3 locations\n",
    "- two eyes: will detect the \"is an eye\" feature in two locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The convolution operation is like\n",
    "- creating a small (size of template) FC layer\n",
    "- that is applied to each location\n",
    "\n",
    "So it \"fully connects\" *neighboring inputs* rather than the *entire input* and thus takes advantage\n",
    "of ordering present in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The template is called a *kernel* or *filter*\n",
    "- The output of a convolution is called a *feature map*\n",
    "\n",
    "Pre-Deep Learning: manually specified filters have a rich history for image recognition.\n",
    "\n",
    "Let's see some in action to get a better intuition.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "<center>Fully Connected vs Convolution</center>\n",
    "<tr>\n",
    "<img src=\"images/CNN_vs_FC.jpg\" width=600\">\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Fully Connected Layer\n",
    "    - each of the $n_{(l-1)}$ units of layer $(l-1)$ connected to each of the $n_\\llp$ units of layer $\\llp$\n",
    "    - $n_{(l-1)} * n_\\llp$ weights total\n",
    "- Convolutional Layer with 1D convolution, filter size 3\n",
    "    - groups of $3$ units of layer $(l-1)$ connected to *individual* units of layer $\\llp$\n",
    "    - using the *same* 3 weights\n",
    "    - 3 weights total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So a Convolutional Layer can use *many fewer* weights/parameters than a Fully Connected Layer.\n",
    "\n",
    "As we will see, this enables us to create *many* separate convolutions in a single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CNN_Helper' object has no attribute 'create_img'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c1ff0e37942d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcnnh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_convs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Notebooks/NYU/cnn_helper.py\u001b[0m in \u001b[0;36mplot_convs\u001b[0;34m(self, img, filters)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         fig, axs = plt.subplots( len(filters), 3, figsize=(12, 12 ),\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CNN_Helper' object has no attribute 'create_img'"
     ]
    }
   ],
   "source": [
    "_= cnnh.plot_convs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A bright element in the output indicates a high, positive dot product\n",
    "- A dark element in the output indicates a low (or highly negative) dot product\n",
    "\n",
    "In our example, the kernel is $(3 \\times 3)$.\n",
    "\n",
    "The template match will be maximized when\n",
    "- high values in the input correspond to high values in the matching location of the template\n",
    "- low values in the input correspond to low values in the matching locations of the template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a list of manually constructed kernels (templates) that have proven useful\n",
    "- [list of filter matrices](https://en.wikipedia.org/wiki/Kernel_(image_processing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- How do we construct a \"good\" kernel ?\n",
    "- How do we decide which one to use ?\n",
    "\n",
    "It all depends on the objective.\n",
    "\n",
    "Machine Learning to the rescue: let an ML algorithm \"learn\" the kernel that is best suited to the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, consider a two layer Sequential model\n",
    "- First layer is a convolution with kernel $\\kernel$\n",
    "- Second layer if a classifier with parameters $\\Theta$\n",
    "\n",
    "Let $\\loss$ be some loss function appropriate to classifcation, e.g, cross entropy.\n",
    "\n",
    "Then our ML Swiss Army Knife (Gradient Descent) solves for the loss-minimizing values of $\\Theta, \\kernel$\n",
    "$$\n",
    "\\Theta, \\kernel = \\argmin{\\Theta, \\kernel} \\loss\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multiple kernels\n",
    "\n",
    "We have thus far seen a *single* kernel, applied to a $N=2$ dimensional input $\\y_{(\\ll-1)}$.\n",
    "\n",
    "The output $\\y_\\llp$ is an $N$ dimensional feature map that identifies the presence/absence\n",
    "of a feature at each element of $\\y_{(\\ll-1)}$.\n",
    "\n",
    "Why not use *multiple* kernels to identify *multiple* features ?\n",
    "- Let Convolutional Layer $\\ll$ have $n_{\\llp,1}$ kernels\n",
    "- The output is $n_{\\llp,1}$ feature maps, one per kernel, identifying the presence/absence of one feature each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is similar in concept to a Fully Connected layer\n",
    "- Let FC layer $\\ll$ have $n_\\llp$ units/neurons\n",
    "- The output is a vector of $n_\\llp$ features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $(n_{(\\ll-1),1} \\times n_{(\\ll-1),2})$ denote the shape of $\\y_{(\\ll-1)}$.\n",
    "\n",
    "If Convolutional Layer $\\ll$ has $n_{\\llp,1}$ kernels\n",
    "- the output shape is $(n_{\\llp,1} \\times n_{(\\ll-1),1} \\times n_{(\\ll-1),2})$\n",
    "\n",
    "That is, the input is replicated $n_\\llp$ times, one per kernel of layer $\\ll$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "       <center>CNN convolution</center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/CNN_feature_map.jpg\" width=600></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It also means that the matrix $\\kernel_\\llp$ representing the kernels at layer $\\ll$\n",
    "has the same number of dimensions as the output\n",
    "- the first dimension is the number of kernels\n",
    "- the rest of the dimensions are the size of each kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Higher dimensional input ($N > 2$)\n",
    "\n",
    "After applying $n_{\\llp,1}$ kernels to $N=2$ dimensional input $\\y_{(\\ll-1)}$\n",
    "of shape\n",
    "$(n_{(\\ll-1),1} \\times n_{(\\ll-1),2})$ \n",
    "we get a three dimensional output of shape $(n_{\\llp,1} \\times n_{(\\ll-1),1} \\times n_{(\\ll-1),2})$.\n",
    "\n",
    "What happens when input $\\y_{(\\ll-1)}$ is $N=3$ dimensional \n",
    "of shape\n",
    "$(n_{(\\ll-1),1} \\times n_{(\\ll-1),2} \\times _{(\\ll-1),3})$ ?\n",
    "\n",
    "- this can easily occur in layer $(\\ll-1)$ is a Convolutional Layer with $n_{(\\ll-1),1}$ kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If $\\y_{(\\ll-1)}$ is the output of a Convolutional Layer\n",
    "- $\\y_{(\\ll-1)}$ has $n_{(\\ll-1),1}$ features over a space of shape $(n_{(\\ll-1),2} \\times _{(\\ll-1),3})$\n",
    "\n",
    "Convolutional Layer $\\ll$ \n",
    "- combines  all $n_{(\\ll-1),1}$ input features at a single \"location\" \n",
    "- into a new synthetic *scalar* feature at the same location in output $\\y_\\llp$\n",
    "\n",
    "This means the output $\\y_\\llp$ is of shape $(n_{\\llp,1} \\times n_{(\\ll-1),2} \\times _{(\\ll-1),3})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This implies that *each kernel* of layer $\\ll$ is of dimension $N_{(\\ll-1)}$\n",
    "\n",
    "For example: if layer $(\\ll -1)$ has $N_{(\\ll -1)} = 3$ dimensions\n",
    "- each kernel of layer $\\ll$ is has 3 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel shape\n",
    "\n",
    "We see that a Convolutional Layer $\\ll$ transforms\n",
    "- an input of dimension $(n_{(\\ll-1),1} \\times n_{(\\ll-1),2} \\times n_{(\\ll-1),3})$\n",
    "- into output with dimension $(n_{\\llp,1} \\times n_{(\\ll-1),2} \\times n_{(\\ll-1),3})$\n",
    "\n",
    "If each kernel of layer $\\ll$ has shape $(k_1 \\times k_2)$ then the kernel matrix $\\kernel_\\llp$ for layer $\\ll$\n",
    "- has shape $(n_{\\llp,1} \\times k_1 \\times k_2)$\n",
    "- one kernel per output synthetic feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple case: 1d convolution\n",
    "\n",
    "We have thus far illustrated Convolution with input layer $0$ having $\\x^\\ip$ of dimension $N_{(0)} \\in \\{2,3 \\}$.\n",
    "\n",
    "We can generalize the logic to tensors of dimension $N > 3$.\n",
    "\n",
    "But we also have the simplest case of $N=1$.\n",
    "- can consider the one dimensional $\\x^\\ip$ has being two dimensional with leading dimension $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One dimensional convolution is quite common\n",
    "- timeseries of prices\n",
    "- sequence of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Consider a time series of prices of length 5\n",
    "- a positive spike at elements 1 and 3\n",
    "- a FC has no order\n",
    "    - can't distinguish between $[+,-+]$ and $[+,+,-]$\n",
    "    - but a 1D convolution with kernel size 3 can\n",
    "\n",
    "Consider a sequence of words\n",
    "- an FC cannot distinguish $[\"not\", \"like\", \"ML\"]$ from $[\"ML\", \"not\", \"like\"]$\n",
    "    - but a 1D convolution with kernel size 3 can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So Convolutional Layers can impose a partial ordering (within range of kernel) where FC Layers cannot.\n",
    "\n",
    "This doesn't completely address the issue of inputs that are sequences as the \"field\"\n",
    "of ordering is only within (a small) kernel.\n",
    "\n",
    "We will learn to deal with sequences when we study Recurrent Neural Networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Technical points\n",
    "\n",
    "## Convolution versus Cross Correlation\n",
    "- math definition of convolution\n",
    "    - dot product of input and *reversed* filter\n",
    "    - we are doing [cross correlation](https://en.wikipedia.org/wiki/Convolution_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Extending the dot product to higher dimensions\n",
    "\n",
    "The \"dot product\" notation $\\kernel \\cdot \\y_{(\\ll-1)}$\n",
    "is extended over higher dimensions\n",
    "- that is: flatten both $\\kernel$ and $\\y_{(\\ll-1)}$ and apply the one dimensional dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Terminology\n",
    "\n",
    "- a *kernel* or *filter* is a pattern\n",
    "    - slide over each element of the input\n",
    "- a *feature map* or *activation map* is the output of applying a single kernel to the input\n",
    "    - similar shape to the input (will discuss padding)\n",
    "    - identifies the presence/absence of a feature *at each* location of the input space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Border control: Padding\n",
    "\n",
    "We have glossed over an important fact:\n",
    "- What happens when the kernel center is located at an edge ?\n",
    "    - That is: part of the kernel lies outside of the input\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Up until now\n",
    "- we have been treating the size of feature map of layer $(\\ll -1)$ and layer $\\ll$\n",
    "as being the same size.\n",
    "\n",
    "In order for that to be true\n",
    "- we need to \"pad\" the borders of the input, usually with 0 values\n",
    "    \n",
    "There are several variants of padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- same, full\n",
    "    - Output feature map the same size as input feature map\n",
    "- valid\n",
    "    - **No** padding\n",
    "    - Output feature map small than input feature map (by half of the kernel size)\n",
    "- causal\n",
    "    - For input that has a temporal ordering: don't peek into the future !\n",
    "        - 1D convolutions\n",
    "        - pad \"earlier\" time with 0; no padding for \"later\"\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Receptive field\n",
    "\n",
    "The *receptive field* of a elements of a feature map\n",
    "- are the Layer 0 (input) features that affect features in the map.\n",
    "\n",
    "\n",
    "For ease of notation:\n",
    "- we assume $N=2$ as the dimension of the kernel\n",
    "- we assume that all $N$ dimensions of the kernel are the same ($f_\\llp$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So we will assume without loss of generality that\n",
    "- the \"height\" and \"width\" of a single kernel kernel is $(f \\times f)$\n",
    "- the full dimensionality of a single layer $\\ll$ kernel is $(n_{(\\ll-1),1} \\times f \\times f)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus the receptive field of a Convolutional Layer at layer $1$ is $(f \\times f)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Increasing the Receptive Field\n",
    "\n",
    "There are several ways to \"widen\" the receptive field\n",
    "- Increasing $f_\\llp$, the size of the kernel\n",
    "- Stacking Convolutional Layers\n",
    "- Stride\n",
    "- Pooling\n",
    "\n",
    "Striding and Pooling also have the effect of reducing the size of the output feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Increase the size of the kernel\n",
    "\n",
    "Although this is the most *obvious* way of increasing the receptive field, we tend to avoid it !\n",
    "\n",
    "We will see the reason shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stacking Convolutional Layers\n",
    "\n",
    "As you go one layer deeper in the NN, the receptive field width and height increase by (2 * *stride*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>CNN receptive field</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/CNN_Receptive_field.jpg\" width=600></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Each grid in Layer 1 refers to the *same* features in Layer 0\n",
    "- The layer 2 feature labelled $i$ is a function of the Layer 1 features labelled $i$\n",
    "- By completing the $(3 \\times 3)$ grid in Layer 2:\n",
    "    - all $(5 \\times 5)$ layer 0 features are touched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|  Layer  | Receptive field |\n",
    "|-- |-- |\n",
    "1 | $(3 \\times 3)$\n",
    "1 | $(5 \\times 5)$\n",
    "1 | $(7 \\times 7)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Strides\n",
    "\n",
    "Thus far, we have slid the kernel over *each* feature of the input feature map.\n",
    "\n",
    "That is: the kernel moves with *stride* $S = 1$\n",
    "\n",
    "Alternatively, we could skip $(S-1)$ features of the input feature map with stride $S > 1$.\n",
    "\n",
    "This will \n",
    "- enlarge the receptive field\n",
    "- decrease the size of the output feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling\n",
    "\n",
    "We can \"down sample\" a feature map by combining features\n",
    "\n",
    "For example: we can replace a $(2 \\times 2)$ region of feature values with a single average value.\n",
    "\n",
    "This \"down sampling\" is called *pooling*.\n",
    "\n",
    "After pooling, each synthetic feature is a function of more than one features of the prior layer.\n",
    "\n",
    "Pooling will\n",
    "- enlarge the receptive field\n",
    "- decrease the size of the pooling layer's output feature map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pooling operations\n",
    "- Average pooling\n",
    "    - average over the volume\n",
    "    \n",
    "- Max pooling\n",
    "    - Max over the volume\n",
    "- K-Max pooling\n",
    "    - select the K largest elements of the volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Size of output\n",
    "\n",
    "We can relate the size of a layer's output feature map to the size of its input feature map:\n",
    "\n",
    "- input $W_i \\times H_i \\times D_i$\n",
    "- $N$: input size $N \\times N$\n",
    "- $F$: filter size $F \\times F$\n",
    "- $S$: stride\n",
    "- $P$: padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "No padding\n",
    "- output size $( (W_i -F)/S +1 ) \\times ( (H_i - F)/S +1 ) \\times D_o$\n",
    "\n",
    "Padding\n",
    "- output size $( (W_i -F +2P)/S +1 ) \\times ( (H_i - F +2P)/S +1 ) \\times D_o$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    \n",
    "Assuming full padding, a layer $\\ll$ Convolutional Layer with $n_{\\llp,1}$ kernels will have output $\\y_\\llp$ dimension\n",
    "- $(n_{\\llp,1} \\times n_{(\\ll-1),2} \\times n_{(\\ll-1),3})$\n",
    "    - $n_{\\llp,1}$ features\n",
    "    - over a spatial map of $(n_{(\\ll-1),2} \\times n_{(\\ll-1),3})$\n",
    "    \n",
    "That is, the number of features changes but the spatial dimension is similar to $\\y_{(\\ll-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Down-sampling: Why does output size matter\n",
    "\n",
    "Convolution applies a kernel to *each* region of the input feature map (assuming $S=1$).\n",
    "\n",
    "Reducing the size of feature map at layer $(\\ll-1)$ \n",
    "- will reduce the number of operations\n",
    "performed by Convolution at layer $\\ll$.\n",
    "\n",
    "For image inputs (with thousands or millions of input features) there is a incentive to down-sample\n",
    "- Speed up training\n",
    "- Speed up inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Down-sample by\n",
    "- Increasing Stride\n",
    "- Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Number of parameters\n",
    "\n",
    "The real power of convolution comes from using the *same* filter against all locations of the input.\n",
    "\n",
    "As a result, the number of parameters is quite small (compared to a separate set of parameters per each input).\n",
    "\n",
    "- Dimension of a single filter $F \\times F \\times D_i$\n",
    "- $D_o$: number of output filters\n",
    "- total parameters: $F * F * D_i * D_o$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember: there is a depth to the input and the filter applies to the entire input depth\n",
    "- size of a filter $F *F * D_i$\n",
    "- number of filters: $D_o$, one per output channel\n",
    "- total: $F * F * D_i * D_o$\n",
    "\n",
    "If we were to have a separate filter for each input location, the number of parameters would increase\n",
    "by a factor of $W_i * H_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Stacking Conv Layers beats larger kernels\n",
    "\n",
    "A single Conv layer kernel with $f=5$ \n",
    "- Has the same receptive field $(5 \\times 5)$\n",
    "- As two stacked layers with kernel size $f=3$.\n",
    "\n",
    "Why might we prefer the latter ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's consider a Convolutional Layer $\\ll$.\n",
    "- The number of features at the input feature map is $n_{(\\ll-1),1}$\n",
    "- The number of features at the output feature map is $n_{\\llp,1}$\n",
    "- The number of weights in each kernel of layer $\\ll$ is $(n_{(\\ll-1),1}\\times f_\\llp)$ (assuming $N=2$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The single Convolutional Layer $\\ll$ with  kernel size $f_\\llp = f$ uses\n",
    "    - $n_{\\llp,1} * (n_{(\\ll-1),1} * f^2)$ weights\n",
    "- Two stacked Convolutional Layers $\\ll, (\\ll+1)$, each with kernel size $f_\\llp = f_{(\\ll +1)} = g$ uses\n",
    "    - $2 * n_{\\llp,1} * (n_{(\\ll-1),1} * g^2)$ weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The single Convolutional Layer uses $\\frac{f^2}{2 g}$ times as many weights.\n",
    "\n",
    "For $f = 5, g=3$: single layer uses almost 3 times (25/9) as many weights !\n",
    "\n",
    "For $f = 7, g=3$: single layer uses almost 5+ times (49/9) as many weights !\n",
    "\n",
    "So it is more parameter efficient to use multiple layers to increase receptive field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Increased depth has another advantage:\n",
    "- each additional layer introduces another non-linear activation\n",
    "\n",
    "Increase non-linearity may result in more complex features being learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Early CNN's tended to use larger kernels.\n",
    "\n",
    "Today, $(3 \\times 3)$ kernels, with many layers, is more common.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kernel size 1\n",
    "\n",
    "Why might one want $f_\\llp =1$\n",
    "- i.e, a $(1 \\times 1)$ kernel ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remember that Convolutional Layer $\\ll$ transforms\n",
    "- an input of dimension $(n_{(\\ll-1),1} \\times n_{(\\ll-1),2} \\times n_{(\\ll-1),3})$\n",
    "- into output with dimension $(n_{\\llp,1} \\times n_{(\\ll-1),2} \\times n_{(\\ll-1),3})$\n",
    "\n",
    "by combining the $(n_{(\\ll-1),1} \\times f_\\llp \\times f_\\llp)$ features of layer ($\\ll -1)$ at a time.\n",
    "\n",
    "Setting $f_\\llp = 1$ combines the $n_{(\\ll-1),1}$ layer $(\\ll-1)$ features at *each* location into a single\n",
    "layer $\\ll feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So $f_\\llp = 1$ is a simple way of reducing the number of features from $n_{(\\ll-1),1}$ to $n_{\\llp,1}$\n",
    "- Using only $(1 * n_{\\llp,1})$ weights !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolution as matrix multiplication\n",
    "[A guide to convolutional arithmetic for deep learning](https://arxiv.org/pdf/1603.07285.pdf)\n",
    "\n",
    "Convolution involves a multi-dimensional dot product over a large volume\n",
    "- each location, of each input feature map\n",
    "\n",
    "Doing this in a loop would be very expensive.\n",
    "\n",
    "There are many highly efficient libraries for matrix multiplication\n",
    "- some of which can take advantage of parallelism and GPU's.\n",
    "\n",
    "Geron equation 13-1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can turn convolution into matrix multiplication.\n",
    "\n",
    "For simplicity, we will show this for a single channel, using a $(3 \\times 3)$ kernel on a $(4 \\times 4 \\times 1)$ input volume.\n",
    "\n",
    "Basically: we flatten out both the kernel matrix $W$ \n",
    "\n",
    "$$\n",
    "W = \\begin{pmatrix}\n",
    "w_{0,0} & w_{0,1} & w_{0,2} \\\\\n",
    "w_{1,0} & w_{1,1} & w_{1,2} \\\\\n",
    "w_{2,0} & w_{2,1} & w_{2,2}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and the input volume matrix.\n",
    "\n",
    "Since the input volume is $(16 \\times 1)$, we will left multiply by a matrix with number of rows equal to the output volume, and $16$ columns.\n",
    "\n",
    "For simplicity, we do this without padding, so the output volume is $(2 \\times 2)$ which flattened is $(4 \\times 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$C = \\begin{pmatrix}\n",
    "    w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "    0       & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 & 0 & 0 & 0 \\\\\n",
    "    0       & 0       & 0       & 0       & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 \\\\\n",
    "    0 & 0       & 0       & 0       & 0       & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} \n",
    "  \\end{pmatrix}\n",
    "  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once you understand that the convolution result\n",
    "- is obtained as $C X'_{l}$ \n",
    "- (where $X'_{l}$ is the flattened inputs to layer $l$), \n",
    "- you can imagine an inverse of $C$ to go from the convolution result\n",
    "backwards to $X'_{l}$.\n",
    "\n",
    "That is, we can trace backwards from each activation in a feature map to the inputs that went into its\n",
    "computation.\n",
    "\n",
    "This will enable us to do back propagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverting convolution\n",
    "\n",
    "For a variety of reasons, it will prove useful to invert the convolution operation.\n",
    "\n",
    "For example\n",
    "- if we view convolution as down-sampling, there will be cases where we want to restore the original volume by up-sampling\n",
    "- we want to know which elements of the input volume contribute to a particular element of the output volume\n",
    "    - need to back propagate the gradient\n",
    "- we may want to know which elements of the input volume contribute most to an element of the output volume (perhaps an output volume several layers removed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will discuss these in the context of understanding what a layer of a CNN is \"looking for\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How many filters to use (what is the correct  number of channels ?)\n",
    " [Bag of Tricks for Image Classification with CNNs](https://arxiv.org/abs/1812.01187)\n",
    " \n",
    "Suppose the kernel size for a CNN layer is $(W \\times H \\times D)$ (thus operating on an input whose channel depth is $D$).\n",
    "\n",
    "Then each convolution dot product is a function of $N = (W*H*D)$ inputs.\n",
    "\n",
    "Having more than $N$ output channels is the opposite of compressing the input: we are generating more\n",
    "values than in the input.\n",
    "\n",
    "So we can argue for $N$ as an  upper bound.\n",
    "\n",
    "This argument is mitigated somewhat by the \"lottery ticket\" argument: \n",
    "- having extra neurons, even if many are eventually \"dead\" (unused, and hence can be pruned), facilitates training\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
