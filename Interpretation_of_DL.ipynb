{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{\\tp}{\\mathbf{{(t)}}}\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\kernel}{\\mathbf{k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import cnn_helper\n",
    "%aimport cnn_helper\n",
    "cnnh = cnn_helper.CNN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does a Deep Learning Classifier work ?\n",
    "\n",
    "We will present the outputs of a very high accuracy classifer for the ImageNet dataset\n",
    "\n",
    "ImageNet:\n",
    "- Large database of hand-labelled images\n",
    "    -14MM images, 22K classes\n",
    "    - [High level categories](http://image-net.org/about-stats_\n",
    "\n",
    "- Annual competition\n",
    "    - no Deep Learning prior to 2012\n",
    "    - drove innovation in Deep Learning post 2012\n",
    "\n",
    "    - Training data: 1.2MM images\n",
    "    - 200 dogs and cats !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/ImageNet_progress.jpg\" width=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the classifier's response to a cat image:\n",
    "<table>\n",
    "<img src=\"images/cat7_classified.png\" width=600>\n",
    "</table>\n",
    "\n",
    "High confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How does the classifier \"recognize\" this as a \"tiger cat\" ?\n",
    "\n",
    "Maybe: by it's parts ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center>How does it work: Parts ?</center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/cat7_shuffle_classified_1.png\" width=450></td>\n",
    "        <td><img src=\"images/cat7_shuffle_classified_2.png\" width=450></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Maybe parts, but certainly not arranged properly !\n",
    "- CNN filter looking for presence/absence of features\n",
    "- not necessarily location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center>How does it work: Parts ?</center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/cat7_occlude_250.png\" width=900></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/cat7_occlude_200.png\" width=900></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Probably not.  Covering up (occluding) various parts still results in correct classification.\n",
    "\n",
    "What about it's shape ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center>How does it work: Shape ?</center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> <img src=\"images/cat7_classified.png\" width=450> <td>\n",
    "        <td><img src=\"images/cat7_silhouette_classified.png\" width=450></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Probably not.\n",
    "\n",
    "Maybe: texture ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <table>\n",
    "    <tr>\n",
    "        <center>How does it work: Texture ?</center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> <img src=\"images/cat7_classified.png\" width=450> <td>\n",
    "        <td><img src=\"images/cat7-elephant1_classified.png\" width=450></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Perhaps it's the texture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is a feature map looking for ?\n",
    "\n",
    "Up until now, our understanding of the workings of a NN has been limited\n",
    "- each layer is a transformation\n",
    "    - from representation (\"synthetic features\") given by output of layer $l-1$\n",
    "    - to a new latent representation, the output of layer $l$\n",
    "- for classification\n",
    "    - the final layer is a logistic regression\n",
    "        - pattern matching the features of penultimate layer\n",
    "        \n",
    "We now begin a quest to understand *what* these transformations are accomplishing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Much of the presentaton is based on a very influential paper\n",
    "by [Zeiler and Fergus](https://arxiv.org/abs/1311.2901)\n",
    "- NYU PhD candidate and advisor !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The first layer\n",
    "\n",
    "It is relatively easy to understand the transformation of the first layer, as its inputs are\n",
    "from our problem domain, which makes them interpretable.\n",
    "\n",
    "For a Dense (FC) layer, we can view the weights as the pattern in the input domain being searched for.\n",
    "\n",
    "This applies as well to the filters in the first convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <center>Layer 1 filters</center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/img_on_page_-004-112.jpg\", width=800\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Each square in the grid represents values of a single filter (\"template\") in Convolutional Layer $1$\n",
    "- The templates seem to represent simple geometric shapes\n",
    "    - lines in various orientations\n",
    "    - colors\n",
    "    - shading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond the first layer\n",
    "\n",
    "Interepretation of filter/weights beyond the first layer is difficult:\n",
    "- layer $\\ll$ takes features from layer $\\ll-1$\n",
    "- which are synthesized and not necessarily interpretable\n",
    "\n",
    "What we can hope to do\n",
    "- somehow map the representation created by layer $\\ll >1$ into the inputs (layer 0 output)\n",
    "\n",
    "The methods fall into two classes\n",
    "- input dependent\n",
    "- input independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interpretation of activations\n",
    "\n",
    "Recall that the output of each layer is a transformed representation of the input.\n",
    "\n",
    "So input $\\x^\\ip$ gets transformed to $\\y_\\llp^\\ip$ at layer $\\ll$.\n",
    "\n",
    "We will give several methods to try to discern the meaning of $\\y_\\llp$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# PCA\n",
    "\n",
    "\n",
    "- Feed all $n$ examples in $\\X$ into the NN\n",
    "    - layer $\\ll$ representation of $\\x^\\ip: \\y^\\ip_\\llp$\n",
    "- Compute PCA of the collection of representations $[ \\y^\\ip_\\llp | 1 \\le i \\le n ]$\n",
    "_ Is there some property $p$ such that\n",
    "    - Project example $i$ onto the first few PC's\n",
    "    - label the projected point with $p(\\x^\\ip)$\n",
    "    - are clusters formed with similar values of property $p$ ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is a Convolutional Neural Network applied to MNIST digit classification.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <center>MNIST CNN</center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/mnist_cnn_pca_0.jpg\" width=800></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And a projection of the representaion produced by the first Convolutional Layer onto the first 2 PC's\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <center>MNIST CNN Conv1 PCA</center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/mnist_cnn_pca_1.jpg\" width=800></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The property we are postulating is useful because \"similar\" digits are clustered together\n",
    "- Is the layer recognizing features that group digits ?\n",
    "- Left to right: strong vertical (\"1\", \"7\") to less vertical ?\n",
    "- Bottom to top: digits *without* \"curved tops\" to those with tops ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's examine the representation after the second Convolutional Layer.\n",
    "<table>\n",
    "    <tr>\n",
    "        <center>MNIST CNN Conv1 PCA</center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/mnist_cnn_pca_2.jpg\" width=800></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interpretation: What is the role of a single neuron/single feature map ?\n",
    "\n",
    "Rather than intepreting $\\y_\\llp$ in its entierty, perhaps we can discern the meaning of a single element\n",
    "$j$ of $\\y_\\llp$.\n",
    "\n",
    "For a given layer $\\ll$, the layer output $\\y_\\llp$ consists of many features.\n",
    "\n",
    "Can we discern what role feature $\\y_{\\llp,j}$ plays ?\n",
    "\n",
    "**Note**\n",
    "If $\\y_\\llp$ is of dimension $n_\\llp > 2$ then $\\y_{\\llp,j}$ denotes a single *feature map*\n",
    "spanning a multi-dimensional space.\n",
    "\n",
    "So when we refer to the value of $\\y_{\\llp,j}$, we mean a summary (e.g., max, average) of all values\n",
    "in the feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximally Activating Examples\n",
    "- Feed all $n$ examples in $\\X$ into the NN\n",
    "- Measure the response $\\y_{\\llp,j}$ \n",
    "- Do the examples with largest/smallest responses share some common property $p$ ?\n",
    "\n",
    "If so, then perhaps $\\y_{\\llp,j}$ encodes a feature measuring the strength of $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let\n",
    "- $\\y_{\\llp,j}^\\ip$ denote the response of feature $\\y_{\\llp,j}$ to input $\\x^\\ip$.\n",
    "- $[ i_1, i_2, \\ldots, i_n ]$ permutation of $[1, \\ldots, n]$ that sorts the responses $[ \\y_{\\llp,j}^\\ip | 1 \\le i \\le n]$\n",
    "    - $\\y_{\\llp,j}^{(i_1)} \\le \\y_{\\llp,j}^{(i_2)} \\dots \\le \\y_{\\llp,j}^{(i_n)}$\n",
    "- Is $p(\\x^{(i^k)})$ true for all $k > T$ for some index $T \\le n$ ?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center>MNIST CNN maximally activating 8's</center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/mnist_cnn_max_activating_8.jpg\" width=800></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Interesting !  Do we have a problem with certain 8's ?\n",
    "\n",
    "Much lower probability when\n",
    "- 8 is thin versus thick\n",
    "- tilted left versus right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Occlusion\n",
    "\n",
    "Maximally activating inputs are very coarse: they identify concepts at the level of entire input.\n",
    "    \n",
    "But, it's reasonable to suspect that some elements of the input are more important to the concept than others.\n",
    "\n",
    "In particluar, a CNN has a \"receptive field\" which defines the input elements that contribute to the layer output.\n",
    "\n",
    "Close to the input layer, the receptive field is narrow so its clear that the \"features\" being identified are small in span."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Occlusion is one way of identifying the elements of the input layer that most affect the latent\n",
    "representation.  \n",
    "\n",
    "We will describe this in terms of a 2D input, but we can generalize.\n",
    "\n",
    "Let\n",
    "- $\\y_{\\llp,j}^\\ip$ denote the response of feature $\\y_{\\llp,j}$ to input $\\x^\\ip$.\n",
    "- Place an occulding square over some portion of input $\\x^\\ip$ and measure the change in $\\y_{\\llp,j}$\n",
    "- Do this for each location in input $\\x^\\ip$ and create a \"heat map\" of changes in response $\\y_{\\llp,j}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The number on top is the percent decrease in $\\y_{(L),j}$, the logit for digit 8.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <center>Occluding 8</center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/mnist_cnn_occlude_8.jpg\" width=800></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Not what we expected !  \n",
    "\n",
    "The mere presence of the square changes the classification probability\n",
    "greatly, even when we are not blocking the \"waist\" of the 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is the change in response of a single feature map in layer 5 of an image classifier (Zeiler and Fergus).\n",
    "\n",
    "The chosen feature map is the one with the highest activation level in the layer.\n",
    "\n",
    "You can see that it is responding to \"faces\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Input image</center></th>\n",
    "        <th><center>Activation of one filter at layer 5</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/img_on_page_-007-139.png\" width=400\"></td>\n",
    "        <td><img src=\"images/img_on_page_-007-148.png\" width=400></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Zeiler and Fergus also measured the change in activation of $\\y_{(L),j}^\\ip$, the logit corresponding to the correct\n",
    "class (\"Afghan Hound\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "      <tr>\n",
    "        <th><center>Input image</center></th>\n",
    "        <th><center>Change in logit for \"Afghan hound\"</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/img_on_page_-007-139.png\" width=400\"></td>\n",
    "        <td><img src=\"images/img_on_page_-007-145.png\" width=400></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- When the dog is masked, logits drop a lot (get colder: blue)\n",
    "- When the two faces are masked, the logits increased (get hotter: red)\n",
    "    - perhaps the faces were competing with the dog for possible classification ?\n",
    "\n",
    "This is super interesting because \"Face\" is *not* in the set of target classes for the training set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This suggests that the CNN \n",
    "- found it useful to recognize faces\n",
    "- even though Face is not an output class\n",
    "- perhaps because Face is *correlated with* some other target class\n",
    "\n",
    "The authors also found this to be true for Text.\n",
    "- not an output class\n",
    "- but strongly correlated with Book, which is a target class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Ascent: Inverting a Neural Network\n",
    "\n",
    "Our use of NN thus far has been to find weights $\\W$\n",
    "- that maximize\n",
    "correctly predicting $\\hat{y} = \\y^\\ip$ given input $\\x^\\ip$.\n",
    "\n",
    "$$\n",
    "\\W = \\argmin{\\W} \\loss\n",
    "$$\n",
    "\n",
    "where $\\loss$ is a measure of the \"distance\" between correct prediction $\\y^\\ip$ and actual prediction $\\hat{\\y}$.\n",
    "- typically MSE for regression\n",
    "- cross entropy for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We solved for the loss minimizing $\\W$ by Gradient Descent on the loss function\n",
    "- Find the gradient of the loss with respect to weights\n",
    "$$\n",
    "\\frac{\\partial \\loss}{\\partial \\W}\n",
    "$$\n",
    "- update weights $\\W$ in the negative direction of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's suppose our trained NN \n",
    "classifies inputs $\\x$ among classes in set $C$.\n",
    "\n",
    "Let $\\W$ be the weights that we obtained.\n",
    "\n",
    "Suppose we *freeze* $\\W$ and present the NN\n",
    "with the following input/target pair\n",
    "$$\n",
    "(\\x^{(i')}, \\y^{(i')}) = (\\x^{(0)}, \\y^{(0)})\n",
    "$$\n",
    "\n",
    "where\n",
    "- $\\x^{(0)}$ is random noise, or all zero\n",
    "- $\\y^{(0)} \\in C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What does the following derivative do ?\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\loss}{\\partial \\x}\n",
    "$$\n",
    "\n",
    "This gradient (with respect to the input layer $\\x$)\n",
    "- shows us the direction in which to modify $\\x$\n",
    "- so that the NN will classify the modified input as $\\y^{(0)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our optimization becomes\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\x = \\argmin{\\x} \\loss^\\ip \\\\\n",
    "\\text{subject to} \\\\\n",
    "\\hat{\\y}^\\ip = \\y^{(0)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is, we are\n",
    "- starting with $\\x = \\x^{(0)} $\n",
    "- modifying $\\x$\n",
    "- to derive an input $\\x$ that causes the NN to predict $\\hat{y} = \\y^{(0)}$ with highest probability\n",
    "\n",
    "We are inverting the output.\n",
    "\n",
    "We refer to this process as *Gradient Ascent* on the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the final $\\x$ is not unique\n",
    "- it is conditioned on the initial  $\\x^{(0)}$\n",
    "\n",
    "We will show how to abuse the NN by choice of $\\x^{(0)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The inverted $\\x$ can be thought of as *the canonical* example of an input with class $\\y^{(0)}$ \n",
    "\n",
    "We can use this $\\x$ as a way of \"discovering\" the input pattern that the NN is seeking to match\n",
    "when classifying an input with label $\\y^{(0)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that we don't have to invert ultimate output $\\hat{y} = \\y_{(L)}$.\n",
    "\n",
    "We can invert any single activation in any layer $\\y_\\llp$.\n",
    "\n",
    "Thus we are \"discovering\" what any individual unit in any layer is seeking to match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is a CNN looking for: Zeiler and Fergus\n",
    "\n",
    " \n",
    "\n",
    "## Input dependent methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Saliency maps/guided back propagation/deconvolution\n",
    "\n",
    "A method similar to occlusion is to just compute the derivative\n",
    "of $\\y_{\\llp,w,h,c}$ with respect to each coordinate $(w,h)$ in input $\\x^\\ip$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\y_{\\llp,w,h,c}}{\\partial \\x^\\ip_{w,h}}\n",
    "$$\n",
    "\n",
    "There are a group of closely related methods along these lines that were discovered independently.\n",
    "\n",
    "The objective of each of these methods is to identify the elements of the input example\n",
    "that most affect an activation (or summary).\n",
    "\n",
    "All the techniques use variants of back propagation on \n",
    "$\n",
    "\\frac{\\partial \\y_{\\llp,w,h,c}}{\\partial \\x^\\ip_{w,h}}\n",
    "$\n",
    "\n",
    "Observe that input elements $\\x^\\ip_{w,h}$ that don't contribute to $\\y_{\\llp,w,h,c}$ have zero \n",
    "derivatives.\n",
    "\n",
    "For those input elements $\\x^\\ip_{w,h}$ that do contribute to $\\y_{\\llp,w,h,c}$ \n",
    "the various methods compute slight variations of the true derivative.\n",
    "Some examples\n",
    "- back-propagate only *positive* intermediate values\n",
    "    - the \"derivative\" identifies only strong positive activations\n",
    "- ignore the channel dimension by back propagating only the maximum (across channels) of the gradient\n",
    "    - so identifies influential elements of the input but can't associate it with a partciluar concept ?\n",
    "\n",
    "It is not so important to understand the details as the concept: identificaton of the elements \n",
    "of input $\\x^\\ip$ that most affect the feature map being probed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Deconvolution and Maximally Activating Patches\n",
    "\n",
    "The Zeiler and Fergus paper uses a form of guided back propagation called *de-convolution* (also know as Convolution Transpose).\n",
    "It's main features\n",
    "- ignores signs of activations\n",
    "    - so looks for \"strong\" signa, either positive or negative\n",
    "    - it is able to go \"backwards\" through max-pooling layers by recording how the forward pass flowed in \"switches\"\n",
    "\n",
    "What Deconvolution will do is find the sub-areas (called \"patches\"_ of the input that influence\n",
    "a feature map.\n",
    "\n",
    "- Similar to Occlusion in that it identifes sub-areas of the input\n",
    "- Different in\n",
    "that it starts at the feature map and works backwards\n",
    "    - rather than starting at the input and working\n",
    "forwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For each layer beyond the first, Zeiler and Fergus show the 9 patches that maximally activate\n",
    "a feature map.\n",
    "That is\n",
    "- a feature map is summarized by the largest absolute value\n",
    "- we find the $N = 9$ inputs responsible for the largest summary of the chosen feature map\n",
    "- we identify the sub-areas (patches) of these image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What we find is\n",
    "- closest to the input, where the receptive field is smallest, primitive geometric features matched\n",
    "    - lines\n",
    "    - shades of color\n",
    "- layer $(\\ll+1)$ combines features of layer $\\ll$ into increasingly complex templates for matching\n",
    "    - edges combined into corners; corners into squares\n",
    "- at higher layers, \"concepts\" start to emerge\n",
    "    - dog face\n",
    "    - text\n",
    "- as the layers get closer to the classifier \"head\", the templates become more specialized for the specific task\n",
    "    - this has implications for Transfer Learning\n",
    "        - too close to the classifier results in task-specific features that may not transfer as well as shallower features\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From Figure 2.  Best viewed under magnification.\n",
    "(we have intentionally scaled the right image to be larger in order to better see its elements)\n",
    "\n",
    "Here are the filters for the first layer, and the patches that activate them.\n",
    "\n",
    "Note: for each filter, the 9 most activating patches are shown.\n",
    "\n",
    "So each grid element on the left corresponds to a a grid element on the right organized as  $3 \\times 3$ patches.\n",
    "\n",
    "For example, the filter at row 7, column 4 (on the left) seems to respond to checked patches (on the right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center>Layer 1 filters</center>\n",
    "    <tr>\n",
    "        <th><center>Filter</center></th> <th><center>Strongly activating image patch for each filter</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/img_on_page_-004-112.jpg\", width=300\"></td>\n",
    "        <td><img src=\"images/img_on_page_-004-114.jpg\", width=900></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From Figure 2.  Best viewed under magnification.\n",
    "\n",
    "Layer 3.\n",
    "\n",
    "Left: Each grid element are the top 9 activations (organized as a $3 \\times 3$ grid) of some feature map,\n",
    "projected to input space.\n",
    "\n",
    "Right: Each grid element is the patch of the activating image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Layer 3 feature map projected to image, for multipe filters</center></th>\n",
    "        <th><center>Strongly activating image patch for each filter</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/img_on_page_-004-115.jpg\" width=600\"></td>\n",
    "        <td><img src=\"images/img_on_page_-004-117.jpg\" width=600></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Magnified view of layer 3 maximally activating patches.\n",
    "\n",
    "<img src=\"images/img_on_page_-004-117.jpg\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Input independent methods\n",
    "\n",
    "All of the preceding methods were specific to a particular input $\\mathbf{x}^{(i)}$.\n",
    "\n",
    "We now describe a method independent of the input.\n",
    "\n",
    "The objective will be to construct a synthetic input $\\mathbf{x}'$\n",
    "that maximizes the value of the activation $\\y_{\\llp,w,h,c}$ of a chosen feature map.\n",
    "\n",
    "One can then try to interpret the activation $\\y_{\\llp,w,h,c}$ (or a summary of the activations in a layer $\\ll$) as attempting to match the synthetic input.\n",
    "Again, if the synthetic input is readily identifiable, we can attempt to infer that the layer\n",
    "is searching for this pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Final word on gradient ascent, with constraints: Cool cost functions\n",
    "\n",
    "Neural style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Further exploration\n",
    "\n",
    "There is a nice video by [Yosinski](https://youtu.be/AgkfIQ4IGaM) which examines the behavior of\n",
    "a Neural Network's layers on video images rather than stills.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
