{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The importance of selectively forgetting\n",
    "\n",
    "The RNN state $\\h_\\tp$ is updated  as a function of the next\n",
    "element $\\x_\\tp$ of the input sequence.\n",
    "\n",
    "Imagine $\\h_\\tp$\n",
    "- as a vector of features\n",
    "- each  feature\n",
    "is an independent function of all previous inputs $\\x_{(\\tt')}$ for $\\tt' \\le \\tt$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But is each and every input $\\x_\\tp$ relevant to all the features in $\\h_\\tp$ ?\n",
    "\n",
    "Probably not.\n",
    "\n",
    "So it would be very powerful\n",
    "- to be able to *selectively update* elements of $\\h_\\tp$ while\n",
    "leaving others unchanged.\n",
    "\n",
    "There are even cases where we might want to reset an element of $\\h_\\tp$ to $0$ (forget)\n",
    "- a new piece of information invalidates prior assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will show how to use math to implement \"conditional\" statements like \"if\" and \"switch\".\n",
    "\n",
    "This will allow us to construct powerful variants of RNN's\n",
    "- that allow selective, independent  modification\n",
    "of features in $\\h_\\tp$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# From Math to Program\n",
    "\n",
    "Early layers (FC, CNN) looked like purely mathematical objects: functions.\n",
    "\n",
    "The RNN is still a (recursively defined) function, but looks more like a program (loop).\n",
    "\n",
    "Later layer types will continue the trend of looking more like programs than mathematical functions\n",
    "- binary conditionals (\"if\" statements)\n",
    "    - gates\n",
    "- multi-case conditionals (\"case\", \"switch\" statements)\n",
    "    - attention\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An important requirement:\n",
    "- the \"program\" must be differentiable in order for the layer to be used in Back Propagation !\n",
    "\n",
    "As we will see, this requirement results in \"soft\" versions of otherwise hard (\"if\" or \"else\") decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## \"If\" statements - Gates\n",
    "\n",
    "The sigmoid function range is $[0,1]$.\n",
    "\n",
    "This makes it attractive to use as a \"gate\" with which to implement an \"if\" statement\n",
    "\n",
    "Suppose we need to compute a $\\y$ that takes on value $T$ if some condition $g$ is `True`\n",
    "and $F$ otherwise.\n",
    "\n",
    "The following product (almost) does the trick\n",
    "\n",
    "$$\n",
    "\\begin{array}[ll]\\\\\n",
    "\\g = \\sigma( \\ldots ) \\\\\n",
    "\\y = \\g \\otimes \\mathbf{T} + (1 -g ) \\otimes \\mathbf{F} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\otimes$ denotes *element-wise* multiplication (Hadamard product)\n",
    "\n",
    "This subtle difference is actually very important: \n",
    "- being element-wise means\n",
    "components of the vectors are independent of one another.\n",
    "- gradients don't interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that $\\y, \\g, \\mathbf{T}, \\mathbf{F}$ are all vectors, not scalars.\n",
    "\n",
    "Essentially: we are making a conditional choice for *each element* of $\\y$, independently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If $\\g$ took on only the values $0$ or $1$, this would exactly replicate a conditional.\n",
    "\n",
    "But $\\g$ wouldn't be differentiable.\n",
    "\n",
    "We use a continous (soft) decision $\\g$\n",
    "- not hard (exactly $0$ or $1$), but in range\n",
    "- hope that $\\y$, which is a mixture of $\\mathbf{T}$ and $\\mathbf{F}$,\n",
    "will be \"mostly\" of the correct type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\y$ being a vector of features means that our pseudo-conditional\n",
    "can update features independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## \"Switch\" statements \n",
    "\n",
    "Let's generalize the binary conditional to multiple choice: a \"switch\" or \"case\" statement.\n",
    "\n",
    "Suppose we need to set $\\y$ to one value from among multiple choices in $\\mathbf{C}$\n",
    "\n",
    "$$\n",
    "\\begin{array}[ll]\\\\\n",
    "\\g = \\sigma( \\ldots ) \\\\\n",
    "\\y = \\g \\otimes \\mathbf{C}  \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the case that $\\g$ were like a OHE (single element with value $1$, all other elements with value $0$) \n",
    "- this would replicate a \"switch\" or \"case\" statement.\n",
    "\n",
    "Analagous with the \"if\" section, this is a soft rather than a hard choice.\n",
    "\n",
    "The \"if\" statement could be derived from the \"switch\" by making \n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\left[\n",
    " \\begin{matrix}\n",
    "    \\mathbf{T}  \\\\\n",
    "    \\mathbf{F}\n",
    " \\end{matrix} \n",
    " \\right] \\\\\n",
    "$$\n",
    "\n",
    "We refer to $\\g$ as a *mask* for $\\mathbf{C}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation for LSTM\n",
    "\n",
    "The vanilla RNN was powerful but limited\n",
    "- gradients were prone to vanishing/exploding\n",
    "- they suffered from \"short term\" memory: \n",
    "    - could not capture dependencies that were too far separated by time\n",
    "\n",
    "LSTM's are one attempt at mitigating these two issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RNNs have a single state vector $\\h$ that serves a dual purpose\n",
    "- a form of memory\n",
    "- a form of transition control: deciding what action to take on the current input $\\x_\\tp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "LSTM's separate these two roles:\n",
    "- a separate \"long-term\" memory vector $\\c$\n",
    "    - can decide at step $\\tt$ what to remember independent of action/output at $\\tt$\n",
    "- a separate \"short term\" memory vector $\\h$, used for transition control \n",
    "    - sole responsibility is deciding on current action/output at $\\tt$\n",
    "    - derived from $\\c$\n",
    "    \n",
    "In addition, the update equations for $\\c$ (and $\\h$ since it is derived from $\\c$) are subtly crafted\n",
    "- to diminish the issue of vanishing/exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The LSTM will seem complicated at first, because it has many small pieces that inter-connect.\n",
    "\n",
    "The following [Blog post by E. Chen](http://blog.echen.me/2017/05/30/exploring-lstms/) is one of the best intuitive\n",
    "explanations that I've seen.\n",
    "\n",
    "Our presentation will follow the \"classical\" derivation (based on the original paper and Geron's book), \n",
    "but with a healthy assist from the blog.\n",
    "                                    \n",
    "                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# State: Long and short term\n",
    "\n",
    "Memory/state is divided within the LSTM into two parts with different roles:\n",
    "\n",
    "- short term state $\\h$ used for \"transition control\"\n",
    "    - analogy: RAM, working memory\n",
    "    \n",
    "- long term state: $\\c$\n",
    "    - analogy: disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The LSTM \"API\"\n",
    "\n",
    "During one time step of computation, the LSTM computes 3 values\n",
    "- new short term state $\\h_\\tp$\n",
    "- new long term state  $\\c_\\tp$\n",
    "- output $\\y_\\tp$ (sometimes simply taken to be same as short term state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The three separate computations are functions of\n",
    "- the previous short term state $\\h_{(\\tt-1)}$,\n",
    "- previous long term state $\\c_{(\\tt-1)}$ \n",
    "- and the current input $x_\\tp$.\n",
    "\n",
    "$$\n",
    "\\y_\\tp , \\h_\\tp, \\c_\\tp = f(\\x_\\tp,  \\h_{(\\tt-1)}, \\c_{(\\tt-1)})\n",
    "$$\n",
    "\n",
    "Note the recursive aspect of the computation of  $\\h_\\tp, \\c_\\tp$: they implicitly depend\n",
    "on the values of the states at all previous time steps $t' < t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rather than produce a distinct output $y_{(t)}$, we identify the output with the short-term state\n",
    "$$\\y_\\tp = \\h_\\tp$$\n",
    "\n",
    "**Geron page 493, last equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Updating long term and short term state\n",
    "\n",
    "## Overview\n",
    "The basic processing step of an LSTM is\n",
    "- update the long term state\n",
    "- derive a new short term state from the updated long term state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are quite a few moving parts to keep track of. \n",
    "\n",
    "Each part is represented by an update equation.\n",
    "\n",
    "The equations differ only in\n",
    "- each has it's own unique weights and bias matrices\n",
    "- the activation functions for gates are $\\sigma$: keeps selectors in range $[0,1]$\n",
    "- the activation functions for memory is $\\tanh$; keeps memory in range $[-1,+1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gates\n",
    "\n",
    "The long term state $\\c$ is a vector of \"features\"\n",
    "- potentially very long\n",
    "- whose elements \"remember\"\n",
    "concepts that prove useful for solving the problem.\n",
    "\n",
    "On each step, it would seem reasonable to\n",
    "- be able to *selectively* update parts of the long state vectors, rather\n",
    "than the entire vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An LSTM associates several gate vectors\n",
    "with the long term state vector.\n",
    "\n",
    "- Each gate vector serves as a mask for the long term state vector, weighting each element.\n",
    "\n",
    "- Thus, it serves as a (continuous) way of selecting sub-parts of the long vectors.\n",
    "\n",
    "There is more than one gate vector because different masks need to be applied during the update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Update long term state\n",
    "\n",
    "The first step in updating the long term state $\\c_\\tp$ is straight forward: \n",
    "- produce a \n",
    "new candidate vector $\\c'_\\tp$\n",
    "to replace the long term state.  \n",
    "- \"candidate\" in that it may or may not replace $\\c_\\tp$\n",
    "\n",
    "The candidate is a function of the prior short term state $\\h_{(t-1)}$ and the current input $\\x_\\tp$:\n",
    "\n",
    "$$\\c'_\\tp = s(\\x_\\tp;  \\h_{(t-1)}) = \\tanh(\\W_{x,c} x_\\tp + \\W_{h,c}\\h_{(t-1)} + \\b_c)$$\n",
    "\n",
    "This is very much like the RNN state update equation, without the recursion (since LHS is $\\c'$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why choose $\\tanh$ as the activation ?\n",
    "- range is $[-1, +1]$\n",
    "- will serve to increment/decrement  a counter ($\\c_\\tp$, as we will see)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We may not want the candidate vector to replace *all* elements of the long term state.\n",
    "\n",
    "The gate vector $\\save_\\tp$\n",
    "- a mask to select only those parts of the candidate\n",
    "that will  replace their counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We may also decide to \"forget\" parts of the long term state as they may no longer be relevant.\n",
    "\n",
    "The gate vector $\\remember_\\tp$\n",
    "- is a mask to select those parts of the *prior long term state vector*\n",
    "which should **not** be forgotten.\n",
    "\n",
    "The  update of $\\c$ combines the old parts to remember with the new parts to include:\n",
    "$$\\c_\\tp = \\remember_\\tp \\otimes \\c_{(t-1)} + \\save_\\tp \\otimes \\c'_\\tp$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note**\n",
    "\n",
    "$\\c_\\tp$ has not been squashed by an activation function, so we haven't limited its range.\n",
    "\n",
    "But we **have** squashed the candidate (via $\\tanh$) so that it's range is $[-1,+1]$\n",
    "\n",
    "So \n",
    "$$\\save_\\tp \\otimes \\c'_\\tp$$\n",
    "acts like an increment/decrement to $\\c_\\tp$.\n",
    "\n",
    "That is: \n",
    "$\\c_\\tp$ acts like a simple counter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Cell state as counter**\n",
    "\n",
    "You can imagine how a counter might be used in a text sequence\n",
    "- Count nesting level\n",
    "    - balanced open/close delimiters\n",
    "    - itemized list counter\n",
    "- Binary counts to determine conditions\n",
    "    - inside/outside a quote\n",
    "    - inside/outside a URL\n",
    "- Count length of input\n",
    "    - end of sentence marker more likely as sentence length increases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Update short term state\n",
    "\n",
    "The short term state update\n",
    "- selects which parts of the newly-updated long term state\n",
    "- to make part of the control program (RAM)\n",
    "$$\\h_\\tp = \\focus_t \\otimes \\tanh(\\c_\\tp)$$\n",
    "\n",
    "That is, the new short term state $h_\\tp$ are (squashed) elements of the new long term state\n",
    "\n",
    "We will also choose to output $\\h_\\tp$:\n",
    "$$\\y_\\tp  = \\h_\\tp$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The gate equations\n",
    "\n",
    "All of the gates are updated via similar equations; each has it's own unique weight and bias matrices.\n",
    "\n",
    "Historically $\\remember_\\tp, \\save_\\tp, \\focus_\\tp$ have been denoted\n",
    "$$f_\\tp, i_\\tp, o_\\tp $$\n",
    "\n",
    "for \"forget\" (although it really means \"don't forget\"!), \"input\", \"output\".\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\remember_\\tp  & = & f_\\tp &  = & \\sigma(\\W_{x,f} \\x_{(t)} + \\W_{h,f}\\h_{(t-1)} + \\b_f) \\\\\n",
    "\\save_\\tp      & = & i_\\tp &  = & \\sigma(\\W_{x,i} \\x_{(t)} + \\W_{h,i}\\h_{(t-1)} + \\b_i) \\\\\n",
    "\\focus_\\tp     & = & o_\\tp &  = & \\sigma(\\W_{x,o} \\x_{(t)} + \\W_{h,o}\\h_{(t-1)} + \\b_o) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These equations are identical except for having their own unique weights and biases.\n",
    "\n",
    "They differ from the \"candidate\" update equation \n",
    "- latter uses a $\\tanh$ activation to squash\n",
    "the long term memory candidate to the range $[-1,+1]$\n",
    "- gates use the sigmoid $\\sigma$ to keep the gates in the $[0,1]$ range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  LSTM as gated residual connections\n",
    "\n",
    "We have previously described how residual (or skip) connections\n",
    "- address the problem\n",
    "of vanishing/exploding gradients\n",
    "- allowing the output of layer $\\ll-1$ to \"skip over\" the computation in layer $\\ll$.\n",
    "\n",
    "This applies to the gradients as well, which flow backwards and can skip a layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Examine the update equation for the long term state:\n",
    "$$\\c_\\tp = \\remember_\\tp \\otimes \\c_{(\\tt-1)} + \\save_\\tp \\otimes \\c'_\\tp$$\n",
    "\n",
    "and consider element $i$ of the vector.\n",
    "\n",
    "If \n",
    "$$\n",
    "\\remember_{\\tp,i} = 1, \\save_{\\tp, i} = 0\n",
    "$$\n",
    "then $\\c_{\\tp,i} = \\c_{(t-1),i}$.\n",
    "\n",
    "That is, the LSTM has the ability to\n",
    "- flow $\\c_{(t-1),i}$ forward  unchanged\n",
    "- and for its derivative to flow backward unchanged\n",
    "\n",
    "This was the key motivation of the skip connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If $\\focus_{\\tp,i}$ is also equal to $1$\n",
    "- the $i^{th}$ component of short term state $\\h_{\\tp,i}$ also \"skips\" interacting at $\\tt$\n",
    "- since\n",
    "$$\\h_\\tp = \\focus_t \\otimes \\tanh(\\c_\\tp)$$\n",
    "\n",
    "So part of the power of the LSTM is \n",
    "- its combination of gates and skip connections to avoid exploding/vanishing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Initial bias to \"not forget\"\n",
    "\n",
    "The  update equation for the long term state:\n",
    "$$\\c_\\tp = \\remember_\\tp \\otimes \\c_{(t-1)} + \\save_\\tp \\otimes \\c'_\\tp$$\n",
    "\n",
    "is not a true \"skip\" connection as it is gated by $\\remember_\\tp$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In practice; we want \n",
    "$$\\remember_\\tp \\approx +1\n",
    "$$\n",
    "in early *epochs* of training.\n",
    "\n",
    "This will speed up learning\n",
    "- by allowing gradients to flow backwards unmodulated\n",
    "- during the time weights need to be adjusted most (from initial, uninformed values)\n",
    "\n",
    "This is done by setting $\\b_f$  to a large value in the equation\n",
    "\n",
    "$$\\remember_\\tp   =  f_\\tp   =  \\sigma(\\W_{x,f} \\x_{(t)} + \\W_{h,f}\\h_{(t-1)} + \\b_f) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention\n",
    "\n",
    "Consider a many to many implementation of a Recurrent NN (RNN, LSTM, etc).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_Encoder_Decoder.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n",
    "​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An example might be a network that adds descriptions/captions to a stream of images (video)\n",
    "- input sequence: a sequence of frames\n",
    "- output sequence: a sequence of words\n",
    "\n",
    "or that translates from one language to another\n",
    "- input sequence: words in source language\n",
    "- output sequence: words in target language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is very possible that the next word (time step $\\tt$) might refer to a much earlier frame ($\\tt' \\lt \\tt)$.\n",
    "\n",
    "A similar thing happens when translating between languages.\n",
    "\n",
    "There is not necessarily a correspondence between output $\\tt$ and input $\\tt$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So an LSTM needs to decide which part of the past to \"attend\" (pay attention) to.\n",
    "\n",
    "We can help it via a mechanism know as \"attention\", which we sketch below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN with Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_Attention.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n",
    "​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The decoder is able to \"select one\" of the prior states, rather than just the latest one.\n",
    "\n",
    "Of course, by now, we understand that this is a \"soft\" select (case/switch)\n",
    "- needs to be differentiable\n",
    "- so it provides a weighted combination of all prior states\n",
    "    - a mask that is almost OHE becomes a true \"choose one\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How does the LSTM decide which of the past states to attend to ?\n",
    "\n",
    "Same way as all Machine Learning:\n",
    "- it is controlled by weights\n",
    "- that are learned by training !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So Deep Learning layers are almost becoming little computers that learn their own programs !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
