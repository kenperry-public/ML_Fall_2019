{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\V}{\\mathbf{V}}\n",
    "\\newcommand{\\v}{\\mathbf{v}}\n",
    "\\newcommand{\\offset}{o}\n",
    "\\newcommand{\\o}{o}\n",
    "\\newcommand{\\E}{\\mathbf{E}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "The datasets for Machine Learning have historically been mainly numeric.\n",
    "\n",
    "But non-numeric data such as Image and Text is an abundant and potentially rich source of insight.\n",
    "\n",
    "We have illustrated many of the concepts in this course with Image data.\n",
    "\n",
    "We will briefly dive into the world of text.\n",
    "  \n",
    "*Natural Language Processing* is the set of tools/techniques that facilitate using text as raw material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The world of text\n",
    "\n",
    "- SEC filing\n",
    "- analyst reports\n",
    "- news articles\n",
    "- tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will approach text mainly from a Deep Learning perspective\n",
    "- lots of data\n",
    "- minimal pre-processing\n",
    "- \"feature engineering\" by the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is not to discount more \"classical\" methods for NLP\n",
    "- Part of speech\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- n-grams\n",
    "\n",
    "All of these are potentially useful as pre-processing steps for Deep Learning.\n",
    "\n",
    "However, if our data sets are big enough, it may be counter-productive to preprocess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Issues with text\n",
    "\n",
    "There are several big issues to tackle regarding text data\n",
    "- Words are categorical variables\n",
    "- Token sequences (sentences/paragraphs) are variable length\n",
    "- Token sequences: order matters\n",
    "\n",
    "We are using the term \"token\" rather than word\n",
    "- tokens may include punctuation, special characters\n",
    "- tokens may be characters rather than entire words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Notation\n",
    "- $\\w$ is a sequence of $n_\\w$ tokens $\\w_{(1)}, \\ldots, \\w_{(n_\\w)}$\n",
    "- each token is an element of vocabulary $\\V: \\w_\\tp \\in \\V, 1 \\le \\tt \\le ||\\w||$\n",
    "    - token $j$ in vocabulary $\\V$ is denoted $\\V_j$\n",
    "- We define two pseuduo-tokens to denote the start/end of the sentence\n",
    "    - $\\w_(0) = \\text{<START>}$\n",
    "    - $\\w_{(n_\\w+1)} =\\text{<END>}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We need a function to convert a token into a numeric vector:\n",
    "$$\\text{rep}: \\text{token} \\mapsto \\mathbb{R}^{n_\\V}\n",
    "$$\n",
    "\n",
    "\n",
    "One Hot Encoding (OHE) and word embeddings are examples of such a function.\n",
    "\n",
    "- For OHE: $n_\\V = ||\\V||$\n",
    "- For Word Embeddings: $n_\\V$ is the dimension of the embedding vector\n",
    "\n",
    "We will extend $\\text{rep}$ to sequences $\\w$:\n",
    "$$\\text{rep}(\\w) = \\left[ \\text{rep}(\\w_\\tp) | 1 \\le \\tt \\le ||\\w||  \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Issue 1: Words are categorical variables\n",
    "\n",
    "We address the first issue relating to text: words are *categorical variables*.\n",
    "\n",
    "By now, we should know to **not** treat categorical variables as ordinals.\n",
    "\n",
    "Let's review the reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Treating a word as an ordinal \n",
    "$$\\text{rep}(w) \\in \\mathbb{R}^1$$ \n",
    "would imply\n",
    "- \"apple\" < \"orange\" is a sensible statement\n",
    "- that this ordering is meaningful to a Machine Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**\n",
    "\n",
    "Linear regression:\n",
    "$$\n",
    "\\y = \\Theta^T \\text{rep}(w)\n",
    "$$\n",
    "\n",
    "Predict $\\y$ given feature vector (attributes) $\\text{rep}(w)$\n",
    "- by learning parameters $\\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose that we tried to encode word $w$ with an integer:  $\\text{rep}(w) = I_w$.\n",
    "- $I_\\text{apple} = 10 * I_\\text{orange}$\n",
    "    - means \"apple\" has 10 times the impact on prediction $\\hat{\\y}$ as \"orange\"\n",
    "    - impact is $\\Theta * I_w$\n",
    "- Re-encoding \"apple\" with a value 10 times larger would make it 10 times more important\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparse Represention of words by One Hot Encoding (OHE)\n",
    "\n",
    "So the natural way of representing a word is as a categorical variable\n",
    "- indictor per word: $\\text{Is}_\\text{apple}$\n",
    "- One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "OHE is a *sparse* representation\n",
    "- length of $\\text{rep}(w)$ is $| \\V |$, yet only a single non-zero element\n",
    "\n",
    "The problem is that there are lots of words !\n",
    "- $|V|$ is large !\n",
    "- $\\text{rep}(\\w)$ length is $|\\w| |\\V|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Issue 2: Word-streams are variable length\n",
    "\n",
    "We are already familiar with two ways of dealing with variable length input\n",
    "- Use a Recurrent model, which handles sequences of arbitrary length\n",
    "- Convert to a fixed length representation using pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixed length representation via pooling\n",
    "\n",
    "One way to deal with a sequence $\\w$ of words is to convert it to a vector  of **fixed length**.\n",
    "\n",
    "Once the length is fixed\n",
    "- Classical and Deep Learning models taking fixed length inputs\n",
    "can work as usual.\n",
    "\n",
    "Doing so usually involves losing the ordering information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bag of Words (BOW): Pooling\n",
    "\n",
    "We define a *reduction* operation $\\text{CBOW}$\n",
    "- convert a sequence $\\w$ of $||\\w||$ elements\n",
    "- each element of length $|| \\text{rep}(\\w_\\tp) ||$\n",
    "- to a fixed length vector of length $|| \\text{rep}(\\w_\\tp) ||$\n",
    "\n",
    "This will necessarily lose token order: this method is called *Bag of Words (BOW)*\n",
    "\n",
    "There are many operators to achieve the reduction, which we will group under the name *pooling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sum/Average\n",
    "$$\n",
    "\\text{CBOW}(\\w) = \\sum_{\\tt=1}^{||\\w ||} {  \\text{rep}(\\w_\\tp) }\n",
    "$$\n",
    "\n",
    "Since $\\w_\\tp$ is a vector, the addition operation is element-wise.\n",
    "\n",
    "So the composite vector for the sequence is the sum of the vectors of each element in the sequence.\n",
    "\n",
    "We can easily turn the Sum into an average by dividing by $||\\w||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Count vectorization:  \n",
    "\n",
    "In the special case that \n",
    "$$\\text{rep}(\\w_\\tp) = \\text{OHE}(\\w_\\tp)\n",
    "$$\n",
    "\n",
    "$\\text{CBOW}(\\w)_j$ is equal to the number of occurrences in sequence $\\w$ of the $j^{th}$ word in $\\V$.\n",
    "\n",
    "This is often called *Count Vectorization*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### TF-IDF\n",
    "\n",
    "Count Vectorization is simple but ignores a basic fact or language\n",
    "- word \"importance\" is often inversely correlated with frequency in $\\V$\n",
    "\n",
    "In English: \n",
    "- the words \"a\", \"the\" and \"is\" are extremely high frequency (so high counts in most $\\w$).\n",
    "- but are so common as to convey little meaning\n",
    "\n",
    "On the other hand, a rare word (or sequence of words) may be very distinctive (\"Machine Learning\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Term Frequency, Inverse Document Frequency (TF-IDF)* \n",
    "- is based on the idea that\n",
    "a word that is *infequent* in the wide corpus\n",
    "- but is frequent in a particular document in\n",
    "the corpus is very meaningul in the context of the document.\n",
    "\n",
    "So a document \n",
    "- in which \"Machine Learning\" occured a disproportionately high (relative to the broad corpus)\n",
    "number of times \n",
    "- is likely to indicate that the document is dealing with the subject of Machine Learning.\n",
    "\n",
    "**Note** A similar idea is behind many Web search algorithms (Google)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "TF-IDF is similar to the Count Vectorizer, but with modified counts \n",
    "that are the product of \n",
    "- the frequency of a word within a single document\n",
    "- the inverse of the frequency of the word relative to all documents\n",
    "\n",
    "- $v$ is a word\n",
    "- $d$ is a document (collection of words) in set of documents $D$\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll]\\\\\n",
    "\\text{tf}(v,d) & = & \\text{frequency of word } v \\text{ in document } d & \\text{(Term Frquency)}\\\\\n",
    "\\text{df}(v) & = & \\text{number of documents that contain word } v \\\\\n",
    "\\text{idf}(v) & = & \\log( \\frac{ ||D|| } { \\text{df}(v) } ) + 1 & \\text{Inverse Document Frequency} \\\\\n",
    "\\\\\n",
    "\\text{tf-idf}(v,d) & = & \\text{tf}(v, d) * \\text{idf}(v) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detour: Sentiment classification notebook on Colab : simple model\n",
    "\n",
    "Classification task\n",
    "- Input: Movie review, as sequence of characters\n",
    "- Label: Positive/Negative\n",
    "\n",
    "[NLP notebook: examine the data](https://colab.research.google.com/github/kenperry-public/ML_Fall_2019/blob/master/Keras_examples_imdb_cnn.ipynb#scrollTo=shHO2IU80XJ7&line=20&uniqifier=1)\n",
    "\n",
    "[NLP notebook: simple model](https://colab.research.google.com/drive/15KZrB_qR63Q3KjLVdaT2BV-NgsdcXo4F#scrollTo=QtvUFJZJ7Oqi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Back from the detour:  summary of SImple Model\n",
    "\n",
    "- One Hot Encoded words\n",
    "    - OHE via an Embedding layer\n",
    "        - Use Embedding as pedagogical device; world's *slowest* way to perform OHE !\n",
    "- Variable length sequence of words\n",
    "- Global Pooling to reduce to fixed length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Issue 3: Ordering matters, first attempt using Convolution\n",
    "\n",
    "Not every text problem requires the complete ordering of words.\n",
    "\n",
    "We will briefly discuss non-ordered methods of dealing with text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural n-grams using Conv1d\n",
    "\n",
    "An *n-gram* is a sequence of $n$ consecutive tokens that encapsulates a single concept (*phrase*)\n",
    "\n",
    "\n",
    "An n-gram captures\n",
    "- multi-token concept\n",
    "    - \"New York City\" versus [ \"New\", \"York\", \"City\" ]\n",
    "- ordering information\n",
    "    - [ \"hard\", \"not\", \"easy\" ] versus [ \"easy\", \"not\", \"hard\" ]\n",
    "    \n",
    "NLP can be enhanced by replacing the subsequence of related words by the n-gram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How does one identify n-grams ? There are two approaches.\n",
    "\n",
    "The first is statistical\n",
    "- joint frequency of the phrase's tokens being higher than the product assuming independence\n",
    "\n",
    "- $\\pr{\\text{\"New York City\"}} > \\pr{\\text{\"New\"}} \\pr{\\text{\"York\"}} \\pr{\\text{\"City\"}}$\n",
    "\n",
    "That is: the frequency of the phrase is greater than the joint probability of its components,\n",
    "assuming independence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The other way is: use Machine Learning !\n",
    "- Discover consecutive $n$ tokens that are useful for some task\n",
    "\n",
    "Using one dimensional convolution with kernel size $n$\n",
    "- the convolution encodes each group of $n$ consecutive tokens (assuming stride 1)\n",
    "- using multiple kernels: we can create an n-gram per kernel that captures some concept\n",
    "\n",
    "That is: we have created a new feature, per kernel, at each location of the text sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "n-grams can capture partial ordering of words (within span $n$).\n",
    "\n",
    "So creating n-grams (with varying $n$) \n",
    "- before applying Pooling \n",
    "- retains local ordering for features within span of $n$ tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>One dimensional convolution</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/NLP_conv1d.jpg\" width=1000></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>One dimensional convolution, multiple kernels</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/NLP_conv1d_multi_kernel.jpg\" width=1000></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Global Pooling</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/NLP_Global_Pooling.jpg\" width=600></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detour: Sentiment classification notebook on Colab : Neural n-grams\n",
    "\n",
    "[NLP notebook: neural n-grams](https://colab.research.google.com/github/kenperry-public/ML_Fall_2019/blob/master/Keras_examples_imdb_cnn.ipynb#scrollTo=LPChvdnzUY9f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Back from the detour:  summary of Neural n-grams\n",
    "\n",
    "- One dimensional convolution over time dimension\n",
    "    - 3-grams\n",
    "- Global Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Issue 1 revisited: Sparse verus dense representation of categoricals\n",
    "\n",
    "## Dense representation of words: Embeddings\n",
    "\n",
    "Sparse encodings, such as OHE\n",
    "- convert a token into a vector of features\n",
    "- where the features are orthogonal: only one is active at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is called a *discrete* representation.\n",
    "\n",
    "Discrete representations have a major drawbacks\n",
    "- they are long\n",
    "    -  $\\text{rep}(\\w)$ length is $||\\w|| * ||\\V||$\n",
    "- there is no meaningful metric of \"distance\" between the representation of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate the \"lack of distance\" issue, let \n",
    "\n",
    "$$\n",
    "\\text{OHE}(w)\n",
    "$$\n",
    "\n",
    "denote the One Hot Encoding of word $w$.\n",
    "\n",
    "Using dot product (cosine similarity) as a measure of similarity\n",
    "\n",
    "| word   | OHE(word) | Similarity |\n",
    "| ---    | ---       | :---:        |\n",
    "| dog   | [1,0,0,0]   | OHE(word) $\\cdot$ OHE(dog)  = 1  |\n",
    "| dogs  | [0,1,0,0]   | OHE(word) $\\cdot$ OHE(dog)  = 0  |\n",
    "| cat   | [0,0,1,0]   | OHE(word) $\\cdot$ OHE(dog)  = 0  |\n",
    "| apple | [0,0,0,1]   | OHE(word) $\\cdot$ OHE(dog)  = 0  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each pair of distinct words has 0 similarity\n",
    "- no recognition of plural form\n",
    "- no recognition of commonality (pets)\n",
    "\n",
    "This is due to the fact that only a single \"feature\" of the OHE is active (non-zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, it's possible that, in reality, there are many \"dimensions\" to a word, for example\n",
    "- singular/plural\n",
    "- entity type, e.g., Person\n",
    "- positive/negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- \"Cats\", \"Dogs\", \"Apples\"\n",
    "    - related by being plural form\n",
    "- \"Cat\", \"Dog\"\n",
    "    - related by being animals\n",
    "- \"good\", \"bad\"\n",
    "    - related by being \"opposites\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus it is not unreasonable to represent a word as a short *dense vector* of features \n",
    "- each feature (vector element) captures a concept\n",
    "- numeric value of element encodes the strength of the word's relation to the concept\n",
    "\n",
    "Ideally the features would be indepenent\n",
    "\n",
    "This is called a *continuous* word representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Doing math with words\n",
    "\n",
    "Let's explore the implication and power of dense vector representation of words.\n",
    "\n",
    "Let $\\v_w$ be the dense vector/embedding for word $w$\n",
    "- captures multiple aspects of a word\n",
    "- where each element of the vector is a nearly-independent aspect\n",
    "- then we can perform interesting mathematical manipulations on word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "| $w$   | $\\v_w$ |\n",
    "| ---    | ---       | \n",
    "| cat   | [.7, .5, .01 ]   \n",
    "| cats   | [.7, .5, .95 ]  \n",
    "| dog   | [.7, .2, .01 ]   \n",
    "| dogs   | [.7, .2, .95 ]\n",
    "| apple   | [.1, .4, .01 ]   \n",
    "| apples   | [.1, .4, .95 ]\n",
    "\n",
    "Does the last dimension encode \"plural form\" ?\n",
    "$$\n",
    "\\v_\\text{cats} - \\v_\\text{cat} \\approx \\v_\\text{dogs} - \\v_\\text{dog} \\approx \\v_\\text{apples} - \\v_\\text{apple}\n",
    "$$\n",
    "\n",
    "If so:\n",
    "$$\n",
    "\\v_\\text{apples} \\approx \\v_\\text{apple} + (\\v_\\text{cats} - \\v_\\text{cat})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word analogies\n",
    "\n",
    "king:man :: ? : queen\n",
    "\n",
    "\n",
    "Let\n",
    "- $\\v_w$ be the dense vector for word $w$\n",
    "- $d(\\v_{w}, \\v_{w'})$ be some measure of the distance between the two vectors $\\v_{w}, \\v_{w'}$\n",
    "    - e.g., ( $1 - \\text{cosine similarity}$ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using the distance metric,  define the set of words in vocabulary $\\V$ that are \"closest\" to a word $w$.\n",
    "\n",
    "Let\n",
    "- $\\text{wv}_{n',d}(\\v_w)$ be the dense vectors of the $n'$ words in $\\V$ closest to word $w$\n",
    "$$\n",
    "\\text{wv}_{n',d}(\\v_w) = \\{ \\v_{w'} | \\text{rank}_V( d(\\v_{w}, \\v_{w'}) ) \\le n' \\}\n",
    "$$\n",
    "- $N_{n',d}(w)$ be the set of $n'$ words associated with $\\text{wv}_{n',d}(\\v_w)$\n",
    "\n",
    "\n",
    "$$\n",
    "N_{n',d}(w) = \\{ w' | w' \\in \\text{wv}_{n',d}(\\v_w) \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can define approximate equality of two words $w, w'$ if they are among the closest words \n",
    "\n",
    "$$\n",
    "w \\approx_{n',d} w' \\; \\; \\text{if } \\w' \\in N_{n',d}(w) \n",
    "$$\n",
    "\n",
    "That is: \n",
    "- word $w$ is approximately equal to word $w'$\n",
    "- if $w'$ is among the $n'$ words closest to $w$ according to distance metric $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we can define word analogies:\n",
    "\n",
    "a:b :: c:d\n",
    "\n",
    "means\n",
    "\n",
    "$$\n",
    "\\v_a - \\v_b  \\approx_{n',d}  \\v_c - \\v_d \n",
    "$$\n",
    "\n",
    "So to solve the word analogy for $c$:\n",
    "$$\n",
    "\\v_c \\approx_{n',d}  \\v_a - \\v_b + \\v_d\n",
    "$$\n",
    "\n",
    "To be concrete:\n",
    "$$\n",
    "\\v_\\text{king} - \\v_\\text{man} + \\v_\\text{woman} \\approx_{n',d} \\v_\\text{queen}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why does adding 2 word vectors work\n",
    "- Mikolov\n",
    "    - Vector for a word reflects its context\n",
    "    - Vector is log probability\n",
    "        - so sum of log probabilities is log of product of probabilities\n",
    "            - product is like a logical \"and\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GloVe: Pre-trained embeddings\n",
    "\n",
    "Fortunately, you don't have to create your own word-embeddings from scratch.\n",
    "\n",
    "There are a number of pre-computed embeddings freely available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "GloVe is a family of word embeddings that have been trained on large corpra\n",
    "- GloVe6b\n",
    "    - Trained on 6 Billion tokens\n",
    "    - 400K words\n",
    "    - Corpus:  Wikipedia (2014) + GigaWord5 (version 5, news wires 1994-2010)\n",
    "    - Many different dense vector lengths to choose from\n",
    "        - 50, 100, 200, 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will illustrate the power of word embeddings using GloVe6b vectors of length $100$.\n",
    "\n",
    "$\n",
    "\\begin{array}[llllll]\\\\\n",
    "\\text{king- man + woman} &  \\approx_{n',d} & \\text{queen } \\\\\n",
    "\\text{man - boy + girl} &  \\approx_{n',d} & \\text{woman } \\\\\n",
    "\\text{Paris - France + Germany} &  \\approx_{n',d} & \\text{Berlin } \\\\\n",
    "\\text{Einstein - science + art} &  \\approx_{n',d} & \\text{Picasso} \\\\\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "You can see that the dense vectors seem to encode \"concepts\", that we can manipulate mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You may discover some unintended bias\n",
    "\n",
    "$\n",
    "\\begin{array}[llllll]\\\\\n",
    "\\text{doctor - man + woman} &  \\approx_{n',d} & \\text{nurse } \\\\\n",
    "\\text{mechanic  - man + woman} &  \\approx_{n',d} & \\text{teacher } \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Domain specific embeddings\n",
    "\n",
    "Do we speak Wikipedia English in this room ?\n",
    "\n",
    "Here are the neighborhoods of some financial terms, according to GloVe:\n",
    "\n",
    "$\n",
    "\\begin{array}[lll]\\\\\n",
    "N(\\text{bull}) & =  & [ \\text{cow, elephant, dog, wolf, pit, bear, rider, lion, horse}] \\\\\n",
    "N(\\text{short}) & =  & [ \\text{rather, instead, making, time, though, well, longer, shorter, long}] \\\\\n",
    "N(\\text{strike}) & =  & [ \\text{workers, struck, action, blow, striking, protest, stoppage, walkout, strikes}] \\\\\n",
    "N(\\text{FX}) & =  & [ \\text{showtime, cnbc, ff, nickelodeon, hbo, wb, cw, vh1}] \\\\\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "It may be desirable to create word embeddings on a narrow (domain specific) corpus.\n",
    "\n",
    "This is not difficult provided you have enough data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Obtaining Dense Vectors: Transfer Learning\n",
    "\n",
    "How do we obtain Dense Vector representation of words ?\n",
    "\n",
    "We learn them !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we had a task T \n",
    "that involves mapping a sequence of words to an outcome.\n",
    "\n",
    "To be concrete: mapping a movie review to an indicator of Positive/Negative sentiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ignoring for the moment the issue of converting variable length sequences to a fixed length\n",
    "- inputs are OHE of words\n",
    "- target is Positive/Negative label\n",
    "\n",
    "- Logistic Regression from  sentence representation to binary target Positive/Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One could also ask\n",
    "- can we map the OHE of a word $\\w_\\tp$ (length $|\\V|$)\n",
    "- to a shorter, dense vector $\\mathbf{e}_\\tp$ of length $n_e$\n",
    "- and use the dense vector in the Logistic Regerssion\n",
    " \n",
    "This mapping can be represented by an an $(|\\V| \\times n_e)$ matrix\n",
    "$\\E$ \n",
    "\n",
    "$$\n",
    "\\mathbf{e}_\\tp = \\text{OHE}(\\w_\\tp)^T \\E \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using Machine Learning, \n",
    "- we solve for both the Logistic Regression parameters $\\W$ *and* $\\mathbf{E}$\n",
    "- when solving the Classification Task via Logistic Regression.\n",
    "\n",
    "The matrix $\\mathbf{E}$ is called \n",
    "- an *embedding matrix* for words \n",
    "- and\n",
    "$\\e_\\tp$ is called an *embedding*  or *word vector* for word $\\w_\\tp$.\n",
    "\n",
    "*Word embeddings* have become an important component of Deep Learning for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Word prediction: Neural Net</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/w2v_word_prediction_layers.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words\n",
    "- we have learned a dense vector representation of words $\\mathbf{E}$\n",
    "- that is useful for a particular classification task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Might it be possible that the dense vector representation of words for this task\n",
    "- is useful for other tasks involving words ?\n",
    "- this is Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problem with this approach is having a large enough training set for the task $T$.\n",
    "\n",
    "We will show how to solve this problem using semi-supervised learning\n",
    "- word prediction problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Word prediction problems: high-level\n",
    "\n",
    "Let's explore how to create generally useful (as opposed to task  specific) word embeddings.\n",
    "\n",
    "In the absence of labelled data (needed for Supervised Learning)\n",
    "- we can create a Semi-Supervised Learning task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From unlabelled sequence $\\w$ define the *word prediction* problem\n",
    "as\n",
    "- predict a target word given a \"context\" sequence of words\n",
    "\n",
    "For example:\n",
    "- given prefix $\\w_{(1)} \\ldots \\w_{(\\tt-1)}$\n",
    "- predict $\\w_\\tp$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The inspiration is that if you can predict the occurrence of word from it's neighbors\n",
    "that  you have somehow capture dimensions of meaning.\n",
    "\n",
    "This is often refered to as \"a word is known by the company it keeps\".\n",
    "\n",
    "- \"I ate an apple\"\n",
    "- \"I ate a  blueberry\"\n",
    "- \"I ate a pie\"\n",
    "\n",
    "\"apple\", \"blueberry\", \"pie\" concept: things that you eat\n",
    "\n",
    "\n",
    "Word embeddings can be obtained as a by-product of this *word prediction* problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Let $\\w$ be the sequence of $n_\\w$ words \n",
    "\n",
    "A *word prediction* is a mapping \n",
    "- from input $\\w$\n",
    "- to a probability distribution $\\hat{\\y}$ over all words in vocabulary $\\V$\n",
    "    - $\\hat{\\y}_j = \\pr{V_j}$\n",
    "    - That is: it assigns a probability to each word in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are some simple word prediction problems:\n",
    "$\n",
    "\\begin{array}[lll]\\\n",
    "\\text{predict next word from context}  & \\pr{\\w_\\tp | & \\w_{(\\tt-\\offset)} \\ldots, \\w_{(\\tt-1)} } \\\\\n",
    "\\text{predict a surrounding word}      & \\pr{\\w_{(\\tt')} |& \\w_\\tp } \\\\\n",
    "    & & \\tt' = \\{ \\tt - o, \\ldots, \\tt + o \\} - \\{ \\tt \\} \\\\\n",
    "\\text{predict center word from context} & \\pr{ \\w_\\tp | & [ \\w_{(\\tt-\\offset)} \\ldots \\w_{(\\tt-1)} \\w_{(\\tt+1)} \\ldots \\w_{(\\tt+\\offset)} ] }  & \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word prediction problems, in detail\n",
    "\n",
    "## Background: Language models\n",
    "\n",
    "A *Language Model* takes a sequence of words and produces a *probability* that\n",
    "the sequence represents a sentence in the language.\n",
    "\n",
    "- We will show how we can obtain this probability by predicting the probability of word $\\w_\\tp$\n",
    "conditional on the first $(\\tt-1)$ words in the sequence.\n",
    "\n",
    "- We will then simplify this by a problem that involves predicting word $w_\\tp$ from\n",
    "a *small* window in the neighborhood of word $\\tt$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Two variants of the window-based approach are the basis for a popular word embedding techinique:\n",
    "word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $\\w$ be the sequence of $n$ words in a sentence.\n",
    "\n",
    "A *language model* \n",
    "- maps $\\w$ into a probability $\\pr{\\w}$ that $\\w$ represents\n",
    "a sentence in the language.\n",
    "\n",
    "We can compute this probability via the chained probabilitiy\n",
    "$$\n",
    "\\pr{\\w} = \\pr{\\w_{(1)} | \\w_{(0)}} \\; \n",
    "\\pr{\\w_{(2)} | \\w_{(0)} \\w{(1)}} \\ldots \\;\n",
    "\\pr{\\w_{(n_\\w+1)} | \\w_{(0)} \\ldots \\w_{(n_\\w)}}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\pr{\\w} = \\prod_{\\tt=1}^{n_\\w+1} { \\pr{\\w_\\tp | \\w_{(0)} \\ldots \\w_{(\\tt-1)} } }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is\n",
    "- the probablility of $\\w$ be a valid sequence of tokens (\"sentence\")\n",
    "- is the chained probability of token $\\w_\\tp$ following prefix $\\w_{(0)} \\ldots \\w_{(\\tt-1)}$\n",
    "    - for each $1 \\le \\tt \\le n_\\w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can determine these probabilities via a *maximum likelihood* estimate by counting \n",
    "word sequence occurrences in our text corpus.\n",
    "\n",
    "$\n",
    "\\pr{\\w_\\tp | \\w_{(0)} \\ldots \\w_{(\\tt-1)} } = \\frac{ \\text{count}_\\text{Corpus} { \\w_0 \\ldots \\w_\\tp} } {\\text{count}_\\text{Corpus} { \\w_0 \\ldots \\w_{(\\tt-1)}}  }\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The estimate via counting is an unrealistic ideal\n",
    "- we don't have *all* possible sentences in the language; the corpus is a subset\n",
    "- may not have *true* probability for rare words in language when corpus is small\n",
    "- computationally expensive\n",
    "- Out of Vocabulary (OOV) problem\n",
    "    - tokens appearing at inference (test) time that were *not* in training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Window based models\n",
    "\n",
    "It is more realistic to *approximate*\n",
    "$$\\pr{\\w_\\tp | \\w_{(0)} \\ldots \\w_{(\\tt-1)} }$$\n",
    "by conditioning $\\w_\\tp$ on a **fixed length** prefix ending at $\\tt-1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Unigram (1-gram) approximation\n",
    "$\n",
    "\\pr{\\w_\\tp | \\w_{(0)} \\ldots \\w_{(\\tt-1)} } \\approx \\pr{\\w_\\tp}\n",
    "$\n",
    "\n",
    "    - That is, conditional probability of $\\w_i$ is just the unconditional probability.\n",
    "\n",
    "- Bigram (2-gram) approximation\n",
    "$\n",
    "\\pr{\\w_\\tp | \\w_{(0)} \\ldots \\w_{(\\tt-1)} } \\approx \\pr{\\w_\\tp | \\w_{(\\tt-1)} }\n",
    "$\n",
    "- n-gram approximation\n",
    "$\n",
    "\\pr{\\w_\\tp | \\w_{(0)} \\ldots \\w_{(\\tt-1)} } \\approx \\pr{\\w_i | \\w_{(\\tt-(n-1))} \\ldots \\w_{(\\tt-1)} }\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can probably see the weakness of the unigram model\n",
    "- Doesn't respect word order\n",
    "$$\\pr{ \\text{[\"New\", \"York\"]} } = \\pr{ \\text{[\"York\", \"New\" ]} }$$\n",
    "\n",
    "and how increasing the window improves the approximation.\n",
    "\n",
    "The assumption that I can predict solely based on a prefix is called a *Markov* assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word prediction problem for word2vec\n",
    "\n",
    "word2vec is based on one of two prediction problems.\n",
    "- predict center word given surrounding words as context\n",
    "- precict which words can occur on either side of a given center words\n",
    "\n",
    "The problems are framed as:\n",
    "Predict target word $w_t$ given conditional word $w_c$\n",
    "- $p(w_t|w_c) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The first prediction problems is called *Skip gram*: predict surrounding words\n",
    "- conditional word is a \"center word\" that is surrounded by other words: $w_c = \\w_\\tp$\n",
    "- target word is any surrounding word at a position $\\tt'$ within a window of $\\tt$\n",
    "    - predict probability of any word $w_t \\in \\V$ being equal to $\\w_{(\\tt')}$ \n",
    "    - where $\\tt' = \\{ \\tt - o, \\ldots, \\tt + o \\} - \\{ \\tt \\}$\n",
    "\n",
    "The set of training examples (example/label pairs) associates with $w_c = \\w_\\tp$\n",
    "$$\\{ (\\w_\\tp, \\w_{(\\tt')}) | \\; \\tt' \\in \\{ \\tt - o, \\ldots, \\tt + o \\} - \\{ \\tt \\} \\}$$\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The second prediction problem is called *CBOW*\n",
    "- center word $\\w_\\tp$\n",
    "- conditional word $w_c$ is any word that surrounds $\\w_\\tp$\n",
    "- target word is the center word $w_t = \\w_\\tp$\n",
    "    - predict probability of $w_t = w_\\tp$ is center word \n",
    "    - given a surrounding word $w_c \\in \\{ \\w_{(\\tt')} | \\tt' = \\{ \\tt - o, \\ldots, \\tt + o \\} - \\{ \\tt \\} \\}$\n",
    "    \n",
    "\n",
    "The set of training examples (example/label pairs) associates with taarget $\\w_\\tp$\n",
    "$$\\{ (\\w_{(\\tt')}, \\w_\\tp) | \\;  \\tt' = \\{ \\tt - o, \\ldots, \\tt + o \\} - \\{ \\tt \\} \\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## word2vec derivation\n",
    "\n",
    "Prediction problem as\n",
    "multinomial (one class per word in vocabulary) classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\y = \\W \\x'$\n",
    "\n",
    "where \n",
    "- $\\y$ is a (OHE) target word \n",
    "- $\\x'$ is derived from (one or more) conditional words.\n",
    "\n",
    "- $\\W$ is $(||V|| \\times ||V||)$\n",
    "    - $\\W^\\ip$, denoting row $i$ of $\\W$ is the \"template\" for target word $\\V_i$\n",
    "        - there are $||V||$ such targets, one per word in $\\V$\n",
    "   -  $\\W^\\ip$ is length $||V||$, the size of the OHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We want to obtain an embedding matrix $\\E$ of dimension $(|\\V| \\times n_e)$\n",
    "- $\\E^{(j)}$, the $j^{th}$ row of $\\E$ is the dense vector of $\\V_j$, the $j^{th}$ word in vocabulary $\\V$.\n",
    "- so $x' = \\o *  \\E$\n",
    "    - where $\\o = \\text{OHE}(\\x)$\n",
    "    - $x'$ is now length $n_e$ rather than $||V||$\n",
    "\n",
    "The regression solves for both $\\W$ (as usual) *and* $\\E$.\n",
    "\n",
    "That is: we find the embedding $\\E$ that is best suited for the classification regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The matrix $\\E$ would then be a map from word $\\V_j$ to embedding $\\e_j$.\n",
    "\n",
    "This embedding matrix would hopefully \"transfer\" to other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's not overlook $\\W$, the matrix of classifier parameters\n",
    "- $\\W^{(j)}$, row $j$ of $\\W$, is the \"template\" for target word $\\V_j$\n",
    "    - multinomial regression has one target per vocabulary word\n",
    "    - it produces a probability distribution $\\hat{\\y}$ over the words in $\\V$\n",
    "    - $\\hat{\\y}_j$ is the probability associated with word $\\V_j$\n",
    "    - length of $\\W^{(j)}$ is $n_e$, same as embedding vector\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In some sense, $\\W^{(j)}$ can *also* be thought of as a dense representation of $\\V_i$\n",
    "- $\\E^{(j)}$ is  representation of $\\V_j$ when it is a conditional word (independent variable)\n",
    "-  $\\W^{(j)}$ is representation of $\\V_j$ when it is a target word (dependent variable)\n",
    "    - i.e., is a template for target $\\V_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is usually the case, for simplicity\n",
    "- to use average of $\\E$ and $\\W$ (which are the same size) as embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Objective function\n",
    "\n",
    "Maximize average log probability over the $T$ examples in training set:\n",
    "\n",
    "$$\\frac{1}{T} \\sum_{\\tt=1}^T { \\sum_{  \\tt' \\in \\{ \\tt - o, \\ldots, \\tt + o \\} - \\{ \\tt \\} } { \\log( p(w_{(\\tt')}|w_\\tp) )} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detour: Sentiment classification notebook on Colab : Learned embeddings\n",
    "\n",
    "[NLP notebook: learned embeddings](https://colab.research.google.com/github/kenperry-public/ML_Fall_2019/blob/master/Keras_examples_imdb_cnn.ipynb#scrollTo=f5XrUD3X8KgP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Back from the detour:  summary of Learned embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Issues 2,3 revisited: Variable length, ordered token sequences\n",
    "\n",
    "We should already have some idea of how to deal with variable length sequences.\n",
    "\n",
    "Recurrent Neural Networks take sequence inputs and create representations (hidden states) that are\n",
    "fixed length \"encodings\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that the RNN produces a sequence of hidden states $\\h{(0)}, \\ldots , \\h_{(||\\w||)}$.\n",
    "\n",
    "Hidden state $\\h_\\tp$ is effectively a summary or encoding of $\\w_{(0)} \\w_{(1)} \\dots \\w_\\tp$\n",
    "- it is computed after having seen the prefix of $\\w$ ending at $\\tt$.\n",
    "\n",
    "So $\\h_\\tp$ is a fixed length encoding of $\\w_{(0)} \\w_{(1)} \\dots \\w_\\tp$.\n",
    "\n",
    "This gives us a way to convert variable length $\\w$ into fixed length $\\h_{(||\\w||)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
