{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{\\tp}{\\mathbf{{(t)}}}\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import neural_net_helper\n",
    "%aimport neural_net_helper\n",
    "\n",
    "nnh = neural_net_helper.NN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer Learning: how to learn from little data\n",
    "\n",
    "The biggest constraint in training a model is obtaining a sufficient amount of training data.\n",
    "\n",
    "The deeper (greater number of layers) your model\n",
    "- the more weights/parameters need to be estimated\n",
    "- increases the quantity of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall our lecture on Interpreting the layers of a Neural Network\n",
    "- layers close to the input seem to learn simple features\n",
    "- layer $\\ll$ creates new features that are combinations of features of layer $(\\ll-1)$\n",
    "\n",
    "Is it possible that we can \"re-use\" feature transformations ?\n",
    "- Use the layers closest to input for a NN trained on a \"source\" Task\n",
    "- But apply these layers (and their transformations on input) to a new \"target\" Task ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Yes !  \n",
    "\n",
    "This is called *Transfer Learning*\n",
    "\n",
    "Create a NN for the new Task by\n",
    "- using some number of layers (closest to input) of a *trained* model for some source task\n",
    "- appending new *untrained* layers for the target task\n",
    "    - final \"head\": regression, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transfer Learning: pre-trained model</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transfer_Learning_1.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Quite often, the Source task's model \n",
    "- has been trained on lots of data\n",
    "- has been trained for large amounts of time\n",
    "    - 2-3 weeks for image models\n",
    "- has a very large number of parameters\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Transfer Learning approach imports the Source task's layer (with weights) at no cost !\n",
    "\n",
    "The new layers added for the Target task might be able to benefit from the feature transformations\n",
    "created by the Source Task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This means\n",
    "- the Target task training modifies *only* the parameters of the new layers\n",
    "    - *freeze* the weights of the imported Source layers\n",
    "- By using a small number of parameters\n",
    "    - Target task can be trained on a small amount of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to choose the prefix of the Source task\n",
    "\n",
    "Where do we truncate the Source task's model ? \n",
    "\n",
    "In other words: how deep should the prefix of the Source model's NN be ?\n",
    "\n",
    "Consider the features created at the final layer of the prefix:\n",
    "- Very shallow\n",
    "    - Features learned may be too simple\n",
    "    - Target may be able to benefit from deeper prefix\n",
    "- Too deep\n",
    "    - Features learned may be *too specialized* to the Source task\n",
    "    \n",
    "In other words: experiment !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transfer Learning: replace the head of the pre-trained model</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transfer_Learning_2.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transfer Learning: replace the head, deep layers of the pre-trained model</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transfer_Learning_3.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n",
    "Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Limitations of Transfer Learning\n",
    "\n",
    "There is no guarantee that the features learned by the Source task will be useful for the Target task\n",
    "- greater chance if tasks/domains are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training the Target task\n",
    "\n",
    "Why do we freeze the weights of the imported Source prefix ?\n",
    "- the weights of the Target task's suffix are uninitialized\n",
    "    - large gradients to start\n",
    "- so early in traing: we don't to destroy the weights in the prefix\n",
    "\n",
    "After the suffix is trained, we sometimes\n",
    "- unfreeze the latter layers of the prefix\n",
    "- train with a *much lower* learning rate than the suffix\n",
    "\n",
    "In other words: we try to \"fine-tune\" the prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The key is fine-tuning\n",
    "- wait until Suffix has been trained enough to generate small gradients\n",
    "- differential learning rates per layer\n",
    "    - the Prefix has been trained on lots of examples\n",
    "    - don't want to alter these weights based on the small number of Target training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer learning in Keras"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "target_model = Sequential()\n",
    "\n",
    "# Import the prefix of source_model\n",
    "# - Import the architecture\n",
    "# - and the weights\n",
    "# Freeze the imported weights\n",
    "for layer in source_model.layer[:num_prefix_layers]:\n",
    "    target_model.add( layer, trainable=False )\n",
    "    \n",
    "# Add the Suffix of the target task to the model\n",
    "target_model.add( ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pre-trained Models in Keras\n",
    "\n",
    "## Image\n",
    "[ImageNet pre-trained models](https://keras.io/applications/)\n",
    "\n",
    "## NLP\n",
    "[Pre-trained word embeddings](https://keras.io/examples/pretrained_word_embeddings/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model zoo\n",
    "[Open source, pre-trained models](https://modelzoo.co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Could probably have used above to split autotencoder into encoder section\n",
    "- Image recognition\n",
    "    - load pre-trained\n",
    "    - cut off head\n",
    "        - where:\n",
    "            - too late: highly specialized\n",
    "                - better to do earlier\n",
    "                - earlier layers learn \"generic features:\n",
    "    - replace head\n",
    "        - train on *small* number of specialized examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
